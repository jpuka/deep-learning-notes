{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applied Machine Learning is an iterative process, test something and see how it works and improve it. In deep learning the hyperparameters are the center of this testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into sets\n",
    "\n",
    "1) A training set is used to train the model  \n",
    "2) A development set is used to see if the trained model performs well  \n",
    "3) A test set is used after the final model has been created through multiple iterations with 1 and 2. It is used for an unbiased evaluation of how good the final model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back in the day, with small datasets (100, 1000 or 10000 examples) a split of 60/20/20 % might be seen. However, this is not necessary with big data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With big data, like a million examples, the split into 1, 2, 3 might be something like 98/1/1 %, since 10000 examples will be enough for dev and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important that the dev and test sets come from the same distribution/source. Would be nice if training set would also be from the same source, but that's not always realistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, if you don't need an unbiased estimate of the model, the test set is not needed. In that case, you would only have 1 and 2. In cases like this, the dev set is sometimes confusingly called \"test set\", even though it really is just a dev set (the model is is basically fit to the \"test set\", so it's not a real test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High bias = underfitting  \n",
    "High variance = overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the training and dev set errors can tell us about bias and variance. If the training error is very large, there is likely high bias. If the difference between the training error and dev error is large (2>>1), there is likely high variance. There can also be both at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple analysis works, if dev and test are from the same distribution and if the Bayes error is close to zero (\"how well would a human classify cats\", images are not blurry)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/bias_var.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For curiosity's sake, this is what a simultaneous high bias and high variance might look like (purple line). Some parts are underfit and others overfit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/hi_bias_var.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic recipe for training neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving bias and variance problems are always specific with the task at hand, but there are some general guidelines to make approaching the problems more systematic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by reducing bias to an acceptable level. Then reduce variance. Do some iterations between these until you are happy. Here are some ideas for reducing bias and variance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/recipe.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"bias variance tradeoff\" is mostly a historical problem, where decreasing one would increase the other. With modern deep learning and related methods, this is not such a big deal anymore, since different methods are used for decreasing bias and variance. Avoiding this tradeoff is one reason, why DL has been so useful in supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as you have a well-regularized network, training a bigger network almost never hurts, it just takes longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization should be one the first attempted solutions to a high-variance problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is done by adding a regularization term to the cost function. Lambda is a regularization (hyper)parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In neural networks, the Frobenius norm (aka L2-regularization) is typically used. It is a sum over squared elements of a matrix. [NOTE: In the following image, the equation for Frob norm is incorrect]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to take the regularization term into account while doing backprop and updating the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/regu.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why regularization reduces overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularization term in the cost function penalizes weight matrices for being too large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a large value for lambda means that many W-terms have to be close to zero. Consequently, many hidden units become insignificant, which creates a \"simpler network\" less prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/y_reg.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Another way to think about this for a tanh activation: setting a high lambda will push the W-parameters close to zero, which pushes the activation function outputs to the linear zone of tanh. A more linear network -> reduced overfitting.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While L2-regularization might be the most popular regularization method, dropout is also a powerful method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout regularization drops out (eliminates) random nodes in the neural network. This basically creates a smaller, diminished network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random dropout is repeated for each training example. So each example has its \"own\" network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverted dropout is the most common way to implement this regularization. After the dropout, *a* is scaled back up by the dropout rate (see green box). If we did not do the inversion step, *a*'s expected value would be lowered by the dropout rate (e.g. *a* would be overall 20 % smaller than it should be). This would cause scaling problems in testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/do_reg.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dropout should not be used during test time (test data), since it would make the testing outputs noisy (random). Luckily, if we used inverted dropout to scale *a* back up while training, we actually do not need to take dropout into account while testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout makes the network \"smaller\". Using a smaller network has a regularizing effect reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout makes singular units less able to rely on singular features, because any feature might get dropped out. This means, that they spread out the weights (instead of one very high and others low). This shrinks the squared norm of the weights. So just like L2-regularization, dropout basically shrinks the weights, albeit in a different way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/y_do.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of keep.prob can also be varied for each layer. Typically you would have a lower keep.prob (so drop out more units) in bigger layers, and a higher keep.prob in smaller layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time you don't want to drop out data from the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computer vision, dropout is commonly used since there is almost never enough data, which causes overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A downside of dropout is that the cost function J is no longer well-defined. By dropping out random nodes, you make J hard to calculate. Consequently, you lose a debugging tool: you can no longer graph the cost function over iterations and expect J to decrease. To combat this, you can momentarily turn off dropout, check that J monotonicly decreases, and then turn dropout back on (and pray there aren't other bugs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other regularization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data augmentation**. Increase the amount of training examples by augmenting the training set: e.g. flipping/cropping/rotating/zooming images and adding them as \"new\" examples. This is not as good as obtaining actually new examples, but it is free and it helps. However, use your brains: you might want to tell your network that a horizontally flipped cat is still a cat, but maybe not a vertically flipped one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Early stopping**. Basically you stop the optimization early while the W-values are in the midrange (they are initialized close to zero). The right time can be figured out with the dev set by stopping when the dev set error starts increasing (usually it first goes down and then increases). However, a big downside is that early stopping messes up orthogonalization by mixing up optimizing J and solving overfitting (making everything more complicated). Usually it's better to just use L2-reg. However, ES might sometimes be more handy than L2, since you don't need to try out values for the hyperparameter lambda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orthogonalization = think of one task at a time (it's this idea/concept). E.g.\n",
    "1) Optimize J.  \n",
    "2) Solve overfitting.  \n",
    "\n",
    "This concept will be talked about more later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard (Ng) way to go is to just use L2-reg and try out different values of lambda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the inputs will speed up the training of your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically you normalize a feature by subtracting the mean $\\mu$ and dividing by standard deviation $\\sigma$. This gives the different features a similar scale (e.g. -1 ... 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same mean and sd calculated for your training set to normalize your dev set and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization works, because it makes the cost function look more symmetric relative to the parameters. This allows the optimization algorithm (gradient descent) to find the J minimum faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/norm.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing/exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In very deep neural networks, the derivatives/slopes can get very small (vanish) or very big (explode), which makes training difficult and slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carefully chosen random weight initialization can significantly reduce this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your weights are lower than 1, in a very deep nn the activations (-> derivatives) can vanish ($W^L$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your weights are higher than 1, in a very deep nn the activations can explode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image example, a linear activation is used to make this example simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/vaex.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initialization for deep networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careful initialization of the weights can partially solve the vanishing/exploding gradient problem. However, this is not a total solution and the problem will still persist to some degree in very deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically we want to initialize the weights so that each of the W-matrices is not too much bigger or smaller than 1. This way z doesn't grow or decrease too quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve this, we can set the variance of the initialized weights to $\\frac{2}{n}$ (ReLU) or $\\frac{1}{n}$ (tanh), where n is the number of incoming features to a neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance can be also taken as a hyperparameter, but usually this is not necessary/worth the trouble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/we_in.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical approximation of gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use numerical derivatives to check if the analytical derivatives in our back propagation are correct -> if we are implementing back prop correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two-sided numerical derivative formula is more accurate than the one-sided one. We will use the two-sided formula for gradient checking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/che_de.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient checking can be used to verify that back propagation is implemented correctly, and to find bugs if it is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically you put all the parameters (W, b) into a single vector $\\theta$. Similarly, you put all the derivatives of the parameters (dW, db) into a single vector $d\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you compute the numerical derivative $\\theta_{approx}$ for each element in the parameter vector $\\theta$ and compare it to the \"real\" derivative in $d\\theta$. If the values are close to one another, then you have implemented back prop correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the values are not close to one another, see which terms have larger differences and search for bugs in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/gra_che.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Checking Implementation Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some practical tips for how to implement gradient checking:  \n",
    "1) Do grad check only to debug - turn it off for standard training. If you always keep it on, it makes training very slow.  \n",
    "2) If grad check fails, then check which terms have a large difference. Check calculation of those terms for bugs.  \n",
    "3) If you are using regularization, then remember to include the J reg term in the calculations while doing grad check.  \n",
    "4) Gradient check does not work with dropout. Dropout makes J definition blurry, so grad check becomes hard to use. If you have to do this, do the grad check with dropout off.  \n",
    "5) Run the grad check at initialization and then later after training the network for a while. It is possible that the grad check passes at initialization since w, b are close to zero but fails with higher values. This is usually not the case, but good to keep in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1 - Programming exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the weights to zeros:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, and you might as well be training a neural network with $n^{[l]}=1$ for every layer, and the network is no more powerful than a linear classifier such as logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you should remember**:\n",
    "- The weights $W^{[l]}$ should be initialized randomly to break symmetry. \n",
    "- It is however okay to initialize the biases $b^{[l]}$ to zeros. Symmetry is still broken so long as $W^{[l]}$ is initialized randomly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the weights to very large random numbers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- The cost starts very high. This is because with large random-valued weights, the last activation (sigmoid) outputs results that are very close to 0 or 1 for some examples, and when it gets that example wrong it incurs a very high loss for that example. Indeed, when $\\log(a^{[3]}) = \\log(0)$, the loss goes to infinity.\n",
    "- Poor initialization can lead to vanishing/exploding gradients, which also slows down the optimization algorithm. \n",
    "- If you train this network longer you will see better results, but initializing with overly large random numbers slows down the optimization.\n",
    "\n",
    "**In summary**:\n",
    "- Initializing weights to very large random values does not work well. \n",
    "- Hopefully intializing with small random values does better. The important question is: how small should be these random values be? Lets find out in the next part! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the weights with He initialization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is named for the first author of He et al., 2015. (If you have heard of \"Xavier initialization\", this is similar except Xavier initialization uses a scaling factor for the weights $W^{[l]}$ of `sqrt(1./layers_dims[l-1])` where He initialization would use `sqrt(2./layers_dims[l-1])`.)\n",
    "\n",
    "Instead of multiplying `np.random.randn(..,..)` by 10, you will multiply it by $\\sqrt{\\frac{2}{\\text{dimension of the previous layer}}}$, which is what He initialization recommends for layers with a ReLU activation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- The model with He initialization separates the blue and the red dots very well in a small number of iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have seen three different types of initializations. For the same number of iterations and same hyperparameters the comparison is:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td>\n",
    "        **Model**\n",
    "        </td>\n",
    "        <td>\n",
    "        **Train accuracy**\n",
    "        </td>\n",
    "        <td>\n",
    "        **Problem/Comment**\n",
    "        </td>\n",
    "    </tr>\n",
    "        <td>\n",
    "        3-layer NN with zeros initialization\n",
    "        </td>\n",
    "        <td>\n",
    "        50%\n",
    "        </td>\n",
    "        <td>\n",
    "        fails to break symmetry\n",
    "        </td>\n",
    "    <tr>\n",
    "        <td>\n",
    "        3-layer NN with large random initialization\n",
    "        </td>\n",
    "        <td>\n",
    "        83%\n",
    "        </td>\n",
    "        <td>\n",
    "        too large weights \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        3-layer NN with He initialization\n",
    "        </td>\n",
    "        <td>\n",
    "        99%\n",
    "        </td>\n",
    "        <td>\n",
    "        recommended method\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you should remember from this notebook**:\n",
    "- Different initializations lead to different results\n",
    "- Random initialization is used to break symmetry and make sure different hidden units can learn different things\n",
    "- Don't initialize to values that are too large\n",
    "- He initialization works well for networks with ReLU activations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2-regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard way to avoid overfitting is called **L2 regularization**. It consists of appropriately modifying your cost function, from:\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}$$\n",
    "To:\n",
    "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- The value of $\\lambda$ is a hyperparameter that you can tune using a dev set.\n",
    "- L2 regularization makes your decision boundary smoother. If $\\lambda$ is too large, it is also possible to \"oversmooth\", resulting in a model with high bias.\n",
    "\n",
    "**What is L2-regularization actually doing?**:\n",
    "\n",
    "L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes. \n",
    "\n",
    "**What you should remember** -- the implications of L2-regularization on:\n",
    "- The cost computation:\n",
    "    - A regularization term is added to the cost\n",
    "- The backpropagation function:\n",
    "    - There are extra terms in the gradients with respect to weight matrices\n",
    "- Weights end up smaller (\"weight decay\"): \n",
    "    - Weights are pushed to smaller values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout** is a widely used regularization technique that is specific to deep learning. \n",
    "**It randomly shuts down some neurons in each iteration.**\n",
    "\n",
    "At each iteration, you shut down (= set to zero) each neuron of a layer with probability $1 - keep\\_prob$ or keep it with probability $keep\\_prob$. The dropped neurons don't contribute to the training in both the forward and backward propagations of the iteration.\n",
    "\n",
    "When you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "- A **common mistake** when using dropout is to use it both in training and testing. You should use dropout (randomly eliminate nodes) only in training. \n",
    "- Deep learning frameworks like [tensorflow](https://www.tensorflow.org/api_docs/python/tf/nn/dropout), [PaddlePaddle](http://doc.paddlepaddle.org/release_doc/0.9.0/doc/ui/api/trainer_config_helpers/attrs.html), [keras](https://keras.io/layers/core/#dropout) or [caffe](http://caffe.berkeleyvision.org/tutorial/layers/dropout.html) come with a dropout layer implementation. Don't stress - you will soon learn some of these frameworks.\n",
    "\n",
    "**What you should remember about dropout:**\n",
    "- Dropout is a regularization technique.\n",
    "- You only use dropout during training. Don't use dropout (randomly eliminate nodes) during test time.\n",
    "- Apply dropout both during forward and backward propagation.\n",
    "- During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here are the results of our three models**: \n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td>\n",
    "        **model**\n",
    "        </td>\n",
    "        <td>\n",
    "        **train accuracy**\n",
    "        </td>\n",
    "        <td>\n",
    "        **test accuracy**\n",
    "        </td>\n",
    "    </tr>\n",
    "        <td>\n",
    "        3-layer NN without regularization\n",
    "        </td>\n",
    "        <td>\n",
    "        95%\n",
    "        </td>\n",
    "        <td>\n",
    "        91.5%\n",
    "        </td>\n",
    "    <tr>\n",
    "        <td>\n",
    "        3-layer NN with L2-regularization\n",
    "        </td>\n",
    "        <td>\n",
    "        94%\n",
    "        </td>\n",
    "        <td>\n",
    "        93%\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        3-layer NN with dropout\n",
    "        </td>\n",
    "        <td>\n",
    "        93%\n",
    "        </td>\n",
    "        <td>\n",
    "        95%\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that regularization hurts training set performance! This is because it limits the ability of the network to overfit to the training set. But since it ultimately gives better test accuracy, it is helping your system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we want you to remember from this notebook**:\n",
    "- Regularization will help you reduce overfitting.\n",
    "- Regularization will drive your weights to lower values.\n",
    "- L2 regularization and Dropout are two very effective regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does gradient checking work?\n",
    "\n",
    "Backpropagation computes the gradients $\\frac{\\partial J}{\\partial \\theta}$, where $\\theta$ denotes the parameters of the model. $J$ is computed using forward propagation and your loss function.\n",
    "\n",
    "Let's look back at the definition of a derivative (or gradient):\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "We know the following:\n",
    "\n",
    "- $\\frac{\\partial J}{\\partial \\theta}$ is what you want to make sure you're computing correctly. \n",
    "- You can compute $J(\\theta + \\varepsilon)$ and $J(\\theta - \\varepsilon)$ (in the case that $\\theta$ is a real number), since you're confident your implementation for $J$ is correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you should remember from this notebook**:\n",
    "- Gradient checking verifies closeness between the gradients from backpropagation and the numerical approximation of the gradient (computed using forward propagation).\n",
    "- Gradient checking is slow, because approximating the gradient with $\\frac{\\partial J}{\\partial \\theta} \\approx  \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$ is computationally costly, so we don't run it in every iteration of training. You would usually run it only to make sure your code is correct, then turn it off and use backprop for the actual learning process. \n",
    "- Gradient Checking, at least as we've presented it, doesn't work with dropout. You would usually run the gradient check algorithm without dropout to make sure your backprop is correct, then add dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning works best on big data, but training on large datasets is slow. Learning to use efficient optimization algorithms is important for speeding up the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the \"standard\" (batch) gradient descent we have to go through all the training examples to take a single gradient descent step. If we have millions of examples, this can be very slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make faster progress, if we allow GD to start taking steps even before processing all the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split your training set (5,000,000 examples) to baby training sets (1,000 examples each). These baby training sets are called **mini-batches**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/mb_gd.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mini-batch gradient descent, we take a step of gradient descent for each mini-batch. So essentially we do everything (forward, cost, backward, update) just as before, just one minibatch at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single pass through the entire training set is called an **epoch**. While in batch gradient descent during a single pass we take a single step, in mini-batch GD during an epoch we take many (here 5000) steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a large dataset, mini-batch is way faster than batch. Mini-batch is the standard GD approach for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/mb_gd_alg.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In batch GD, the cost decreases with each step of GD. However, in mini-batch GD, this does not necessarily happen, since each step is taken based on a different training set. The general trend is still downwards for mini-batch, but the progress is \"noisier\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mini-batch size affects how GD converges:  \n",
    " - If the size is *m*, we basically have the familiar batch GD. Each step is slow.  \n",
    " - If the size is 1, each example is a new \"training set\" and we have **stochastic gradient descent**. Each step is quick, but we lose all the speedup from vectorization, which is inefficient (main problem). It is also very noisy and never converges.  \n",
    " - In practice the size is chosen between m (too large) and 1 (too small). This will give us fastest learning, since we get quick progression (steps) AND the speedup from vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/mb_gd_und.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to choose the mini-batch size?  \n",
    " - For a small training set (m < 2000), use standard batch GD  \n",
    " - For a bigger training set, typical sizes would be between 64 and 512.\n",
    "     - These are usually powers of two (64, 128, 256, 512), because on certain systems this can run faster.\n",
    "     - Find an efficient value through trial and error.\n",
    " - Make sure that the mini-batch fits in the CPU/GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are even more efficient optimization algorithms than batch/mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponentially weighted averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the algorithms that are faster than GD, we need to understand exponentially weighted averages (sometimes called running averages). EWAs are a key component in many advanced optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the equation in the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\beta$ is high (0.98), the exponentially weighted average changes more slowly (green curve), since it puts more emphasis on the previous days' temperatures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\beta$ is low (0.50), the changes are much quicker and noisier (yellow curve), since we are averaging over a shorter time window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/exp_wa.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding exponentially weighted averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically this is quite simple. Basically, $\\beta$ determines how many previous terms actually matter, i.e. how far back does the average go. If beta is higher, more previous terms are added before the function becomes practically zero. So at each time step there is an exponentially decaying function that depends on the previous time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/ewa_und.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation is light on memory, since it requires just keeping a single real number in memory. This is also the main reason why we use this - there are more accurate ways to count a running average, but most of them require more memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.g. you could always sum over 50 days and divide by 50, but this requires keeping more values in memory and is computationally more expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EWA is a simple, computationally and memory efficient way to calculate averages over a lot of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/ewa_imp.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias correction in exponentially weighted averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias correction helps us get a better estimate of <insert quantity\\> (e.g. temperature) in the early stages of EWA. Without bias correction, the values are much lower (purple) than they should be (green, with bias correction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias correction is basically a denominator that goes to one as *t* grows. So it helps early on, then fades away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/ewa_bias.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost always faster than the standard (batch/mini-batch) gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic idea: compute an exponentially weighted average over the gradients and use it to update the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum basically smooths out the steps of gradient descent, which allows GD to find the optimum faster when the cost function has a highly non-symmetric shape. The default GD would just dance around (blue line in image) in cases likes this, but momentum goes straight through (red line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically makes the current step of gradient descent depend on the previous steps. *NOTE: Reminds me of MCMC*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An analogy is a ball rolling into a bowl (see image). There is an acceleration term, the current velocity and the friction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/gd_mom.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation:\n",
    " - A robust default value is $\\beta = 0.90$. \n",
    " - Bias correction is usually not used, since after 10 steps the bias is already gone (with default $\\beta$).\n",
    " - Some people leave out the $(1-\\beta)$ term but we will use it. The different versions just require different $\\alpha$.\n",
    " - $v_{dW}$ has the same dimensions as dW and W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/gd_momi.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root mean square prop has a basic idea similar to momentum, but the equations are slightly different. RMSprop damps down the oscillations in GD allowing us to use a larger learning rate $\\alpha$. All of this speeds up the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image, the updates in the vertical direction are divided by large numbers, which helps damp down the up-down oscillations. In horizontal the division is done by small numbers, so the steps are bigger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small epsilon term is added to the denominator in the update to avoid division by zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/rmsprop.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Adaptive moment estimation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam = Momentum + RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam is commonly used and very effective. It generalizes well for different types of problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historically in DL research there have been a bunch of optimization algorithms that work for very specific tasks making them not very useful generally. But Adam is one of the good guys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Adam, you typically do implement bias correction. Both for the momentum and RMSprop terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/adam.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While using Adam, the hyperparameters are usually chosen as follows:\n",
    " - Tune learning rate $\\alpha$.\n",
    " - Use the default value $\\beta_1 = 0.90$\n",
    " - Use the default value $\\beta_2 = 0.999$\n",
    " - Use the default value (can differ, not super crucial) $\\epsilon = 10^{-8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The betas can naturally also be tuned, but this is rarely done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slowly reducing the learning rate over time could help speed up the training. This is called learning rate decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the initial stages of learning we take bigger steps. As learning approaches convergence, having a smaller learning rate allows us to take smaller steps and get closer to the optimum (especially with something noisy like mini-batch GD with small mini-batches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While implementing decay, the learning rate is usually tied to the epoch number. Alpha_zero, the initial learning rate, and the decay rate are hyperparameters, that usually need to be tuned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/lr_dec.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also other ways to implement learning rate decay, e.g. exponential decay, discrete staircase, manual decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate decay is not among the first options to speed up training. Usually, a well chosen static alpha is already good enough. However, in some cases decay can help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem of local optima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historically, getting stuck to local optima have been thought of as a big problem. This is largely based on thinking about optimization as a low-dimensional problem (like 2-D or 3-D). However, this thinking does not really transfer to high-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays, neural networks might have thousands of parameters. In a parameter space like this, its highly unlikely that there will actually exist a local optima, a point which is flat to all thousands of directions. Usually, every point has some downhills and uphills around it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, while the optimization is not likely to get _stuck_, it can be _slow_. *Plateaus* are regions where the gradient is close to zero for a long time causing the optimization to ṕrogress very slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/plateaus.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern optimization algorithms like Momentum, RMSprop and Adam deal reasonably well with plateaus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2 - Programming exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you should remember**:\n",
    "- The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step.\n",
    "- You have to tune a learning rate hyperparameter $\\alpha$.\n",
    "- With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Mini-Batch Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two steps to build mini-batches from the training set (X, Y).:\n",
    "- **Shuffle**: Create a shuffled version of the training set (X, Y). Each column of X and Y represents a training example. Note that the random shuffling is done synchronously between X and Y. Such that after the shuffling the $i^{th}$ column of X is the example corresponding to the $i^{th}$ label in Y. The shuffling step ensures that examples will be split randomly into different mini-batches. \n",
    "\n",
    "- **Partition**: Partition the shuffled (X, Y) into mini-batches of size `mini_batch_size` (here 64). Note that the number of training examples is not always divisible by `mini_batch_size`. The last mini batch might be smaller, but you don't need to worry about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you should remember**:\n",
    "- Shuffling and Partitioning are the two steps required to build mini-batches\n",
    "- Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will \"oscillate\" toward convergence. Using momentum can reduce these oscillations. \n",
    "\n",
    "Momentum takes into account the past gradients to smooth out the update. We will store the 'direction' of the previous gradients in the variable $v$. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of $v$ as the \"velocity\" of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The momentum update rule is, for $l = 1, ..., L$: \n",
    "\n",
    "$$ \\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n",
    "\\end{cases}\\tag{3}$$\n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\n",
    "b^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}} \n",
    "\\end{cases}\\tag{4}$$\n",
    "\n",
    "where L is the number of layers, $\\beta$ is the momentum and $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that:\n",
    "- The velocity is initialized with zeros. So the algorithm will take a few iterations to \"build up\" velocity and start to take bigger steps.\n",
    "- If $\\beta = 0$, then this just becomes standard gradient descent without momentum. \n",
    "\n",
    "**How do you choose $\\beta$?**\n",
    "\n",
    "- The larger the momentum $\\beta$ is, the smoother the update because the more we take the past gradients into account. But if $\\beta$ is too big, it could also smooth out the updates too much. \n",
    "- Common values for $\\beta$ range from 0.8 to 0.999. If you don't feel inclined to tune this, $\\beta = 0.9$ is often a reasonable default. \n",
    "- Tuning the optimal $\\beta$ for your model might need trying several values to see what works best in term of reducing the value of the cost function $J$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you should remember**:\n",
    "- Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.\n",
    "- You have to tune a momentum hyperparameter $\\beta$ and a learning rate $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - Adam\n",
    "\n",
    "Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp and Momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does Adam work?**\n",
    "1. It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). \n",
    "2. It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). \n",
    "3. It updates parameters in a direction based on combining information from \"1\" and \"2\".\n",
    "\n",
    "The update rule is, for $l = 1, ..., L$: \n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\n",
    "v^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\n",
    "s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\n",
    "s^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n",
    "\\end{cases}$$\n",
    "where:\n",
    "- t counts the number of steps taken of Adam \n",
    "- L is the number of layers\n",
    "- $\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages. \n",
    "- $\\alpha$ is the learning rate\n",
    "- $\\varepsilon$ is a very small number to avoid dividing by zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 - Model with different optimization algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td>\n",
    "        **optimization method**\n",
    "        </td>\n",
    "        <td>\n",
    "        **accuracy**\n",
    "        </td>\n",
    "        <td>\n",
    "        **cost shape**\n",
    "        </td>\n",
    "    </tr>\n",
    "        <td>\n",
    "        Gradient descent\n",
    "        </td>\n",
    "        <td>\n",
    "        79.7%\n",
    "        </td>\n",
    "        <td>\n",
    "        oscillations\n",
    "        </td>\n",
    "    <tr>\n",
    "        <td>\n",
    "        Momentum\n",
    "        </td>\n",
    "        <td>\n",
    "        79.7%\n",
    "        </td>\n",
    "        <td>\n",
    "        oscillations\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        Adam\n",
    "        </td>\n",
    "        <td>\n",
    "        94%\n",
    "        </td>\n",
    "        <td>\n",
    "        smoother\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligible. Also, the huge oscillations you see in the cost come from the fact that some minibatches are more difficult thans others for the optimization algorithm.\n",
    "\n",
    "Adam on the other hand, clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you've seen that Adam converges a lot faster.\n",
    "\n",
    "Some advantages of Adam include:\n",
    "- Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum) \n",
    "- Usually works well even with little tuning of hyperparameters (except $\\alpha$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**:\n",
    "\n",
    "- Adam paper: https://arxiv.org/pdf/1412.6980.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning: tuning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sheer number of hyperparameters can make training networks tricky. The following might be present:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Hyperparameter | Tuning importance |\n",
    "|:---|:---|\n",
    "|$\\alpha$|1st|\n",
    "|$\\beta$|2nd|\n",
    "|\\# of hidden units|2nd|\n",
    "|mini-batch size|2nd|\n",
    "|\\# of layers|3rd|\n",
    "|learning rate decay|3rd|\n",
    "|$\\beta_1, \\beta_2, \\epsilon$|don't tune, use defaults (Adam)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the order of importance is somewhat subjective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When sampling hyperparameter values, do not sample from a symmetric grid. Always sample randomly. This gives you more unique values for each parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/hp_tune.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also follow the \"coarse to fine\" approach. First, randomly sample the entire space. Then, sample again focusing on the region of the space that yielded the best values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning: Using an appropriate scale to pick hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters should be sampled randomly. But the randomness should not always be uniform over the entire range of valid values. Instead, we should pick an appropriate scale for exploring the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hyperparameters like *# of hidden units* and *# of layers*, sampling uniformly on a linear scale over some range can be reasonable. However, this is not the case for all of the hpars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While searching for $\\alpha$, a logarithmic scale should be used. On a linear scale, the randomization does not treat all regions of the space smartly. E.g. when sampling uniformly on a linear scale in [0.0001 ... 1.0], the vast majority of the sampled values will be between 0.1 and 1.0. And we do want to search also in the lower values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/hp_scale.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While searching for $\\beta$ (hpar for exponentially weighted average), a logarithmic scale should also be used. We want to sample more densely in the region, where $\\beta$ is close to one, since that's where the function $\\frac{1}{1-\\beta}$ sees the most rapid changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning in practice: Pandas vs. Caviar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are at least two common approaches for organizing the hyperparameter search process:\n",
    "\n",
    "- Babysit a single model, checking progress and nudging the hpar values every day. Use, if lacking computational resources. Ng: \"Panda\". \n",
    "- Train multiple models in parallel with different hyperparameter settings. Use, if computational resources available. Ng: \"Caviar\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter values from one application domain (e.g. vision) in DL do not automatically work for another (e.g. speech)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization: Normalizing activations in a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization makes the hyperparameter search problem much easier and makes our neural network more robust. It will also enable us to much more easily and efficiently train even deeper networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the input features can speed up learning, as we learned before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out we can extend the normalization to the NN activations to make training even more efficient. This is called batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will normalize the values before activations, i.e. normalize $z$. This is the standard. The normalization can also be done after activation (to $a$), but this is not as common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement batch normalization, we calculate the $z$ mean and sd for a layer and use those to normalize the $z$ values within the layer. This is done separately for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this will force each unit's $z$ to have mean zero and variance one, which we might not want. In other words, we might not want to only give the non-linear activation functions values distributed around zero. Instead, we want to give them a variety of inputs. Consequently, instead of using the $z_{norm}$ directly we will use $\\widetilde{z}$, which can have any mean and sd dictated by the learnable parameters $\\gamma$ and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This $\\beta$ is NOT the same one as used as a hyperparameter for momentum. They just happen to have the same symbol (in their original papers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/batch_norm.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization: Fitting Batch Norm into a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, batch normalization is just added as an additional step between calculating $z$ and $a$ for each unit. Basically, instead of using the unnormalized $z$, we are using the normalized $\\widetilde{z}$ for calculating the activation. Everything else is business as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BN adds two new trainable parameters for each layer: $\\beta$ and $\\gamma$. These are optimized as usual along the others with GD, Adam etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the parameter $b$ can be emitted in BN since the normalization step (mean to zero) makes it redundant. The parameter $\\beta$ sort of takes its place in BN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, batch norm is usually applied with mini-batches, not the entire training set. So basically whenever you calculate the BN, you just use the data from the current mini-batch for the mean and sd. The $\\beta$ and $\\gamma$ are still \"global\" and optimized as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually implementing batch normalization is just a single line of code in a DL framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/bn_imp.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization: Why does Batch Norm work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three major reasons for why batch norm is useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First*, it gives a similar range of values to all the hidden units. So some don't have $z$ from 0 to 1 and others from 0 to 10,000. This speeds up training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Second*, it makes weights deep in the network (e.g. layer 10) more robust to changes to weights in the earlier layers (e.g. layer 1), since the earlier layers cannot shift around as much. This makes learning easier for the later layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the perspective of a later layer, the values coming (as inputs) from the previous layer are changing all the time during training (bc the parameters are changing). Usually, this makes the layer suffer from the problem of *covariate shift*. However, using BN stabilises the distribution of the values coming in from the previous layer - they get a somewhat stable mean and sd. So, BN reduces the problem of the input values of the later layer changing, in essence allowing the later layer to learn more independently of the previous layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Third*, batch norm has a slight regularization effect when used with mini-batches. This is basically a side-effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each $z$ value is scaled by the mean and sd of the mini-batch instead of the whole set. The mean and sd are a bit noisy, since the whole set is not used. This ends up adding slight noise to the value $\\widetilde{z}$. Which in turn adds noise to each layer's activations. This causes a regularization effect, since the later hidden units can not heavily rely on the value of a singular earlier unit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A higher mini-batch size reduces the regulatization effect, logically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, BN should not be used as \"true\" regularization, since the amount of noise is small and it causes only a slight reg effect. For a stronger regularization, use dropout or another reg method alongside BN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization: Batch Norm at test time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch norm processes the data one mini-batch at a time, but at test time we might need to process one example at a time. This requires some adaptation at test time, since the mean and sd are required for getting results but it does not make sense to calculate them based on a single example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this, we estimate the mean and sd from the training set. This is usually done with an exponentially weighted average over mini-batches, which keeps track of the mean and sd values during training. This yields a rough estimate of the mean and sd, which we can use at test time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/bn_test.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, frameworks like Tensorflow usually take care of implementing this, so we rarely have to do it ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class classification: Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been doing binary classifications: \"cat\" or \"no cat\". Softmax regression is a generalization of logistic regression that allows us to recognize multiple classes: \"other\", \"cat\", \"dog\" or \"chicken\". Here we would have four classes or C = 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In softmax regression, the output layer is a *softmax layer* that tells us the probability of each of the classes. P(other), P(cat), ... These sum to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax activation basically exponentiates $z$ and \"normalizes\" with the sum of all the exponentiated values ($e^z$). This yields the probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/smax.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest possible case (no hidden layers), softmax just creates linear decision boundaries between the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class classification: Training a softmax classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multi-class classification problems, the label vector will consist of 0s and 1s. During training, we basically try to optimize the probability of the true prediction (the one that has a 1 in the label vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/smax_t.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this level, you usually just focus on forward prop and rely on some DL framework to implement backprop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we implemented everything manually to understand what's actually going on behind the scenes. But realistically, this is not always the way to go, especially with bigger and more complex neural nets. Instead, it is usually much more practical to build the work on existing deep learning frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/framew.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3 - Programming exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMY1FYvOT13H"
   },
   "source": [
    "**What you should remember**:\n",
    "- Tensorflow is a programming framework used in deep learning\n",
    "- The two main object classes in tensorflow are Tensors and Operators. \n",
    "- When you code in tensorflow you have to take the following steps:\n",
    "    - Create a graph containing Tensors (Variables, Placeholders ...) and Operations (tf.matmul, tf.add, ...)\n",
    "    - Create a session\n",
    "    - Initialize the session\n",
    "    - Run the session to execute the graph\n",
    "- You can execute the graph multiple times as you've seen in model()\n",
    "- The backpropagation and optimization is automatically done when running the session on the \"optimizer\" object."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

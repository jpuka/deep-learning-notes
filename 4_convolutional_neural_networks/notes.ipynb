{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "informal-specific",
   "metadata": {},
   "source": [
    "# Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-indian",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-controversy",
   "metadata": {},
   "source": [
    "## Computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-gibson",
   "metadata": {},
   "source": [
    "Computer vision is developing fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-stanley",
   "metadata": {},
   "source": [
    "Some examples of computer vision: *image classification*, *object detection* (draw boxes around objects in pictures), *neural style transfer* (combining photos with paintings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-scientist",
   "metadata": {},
   "source": [
    "**A single one megapixel image has 3 million features.** If we use a fully connected network, like in the previous courses, learning on such data will require a great deal of computational resources - **we would need a separate parameter for each feature. The *convolution operation* helps us handle large images.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-chambers",
   "metadata": {},
   "source": [
    "## Edge Detection Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-cyprus",
   "metadata": {},
   "source": [
    "As we remember, **in a facial recognition NN the early layers detect edges, later layers detect objects and last layers detect faces**. We will now discuss the edge detection part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-automation",
   "metadata": {},
   "source": [
    "**Filter = kernel**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-secondary",
   "metadata": {},
   "source": [
    "Here is an example of *convolving* (\\*) an image with a filter:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-kuwait",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/ede.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-aquarium",
   "metadata": {},
   "source": [
    "In the case below edge detection basically works by applying a filter that recognizes the difference between \"bright\" and \"dark\" pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-maker",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/ede2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-mobile",
   "metadata": {},
   "source": [
    "This is how the convolution operator works. Next, we will see how it can be used in NNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-answer",
   "metadata": {},
   "source": [
    "## More Edge Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-briefs",
   "metadata": {},
   "source": [
    "**There is difference in the outputs of detecting light to dark and dark to light edge transitions.** With the filter in the image above, the former produces positive and latter negative edges (30 vs -30)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-tribe",
   "metadata": {},
   "source": [
    "Horizontal edge detection works similarly to vertical:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-attribute",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/med.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-percentage",
   "metadata": {},
   "source": [
    "Many types of filters have been invented, for example the Sobel filter puts more emphasis in the middle. However, **nowadays filters are not really coded manually but learned by treating the filter numbers as parameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-triangle",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/med2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-street",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-plane",
   "metadata": {},
   "source": [
    "There are two downsides to applying the convolution as we did above:  \n",
    "1) The image shrinks with every operation (6x6 -> 4x4).  \n",
    "2) The pixels in the corners are only used in a single operation, whereas the pixels in the middle are used multiple times. This basically discards information near the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-burner",
   "metadata": {},
   "source": [
    "***Padding* solves both of these problems. Basically we *pad* an image by adding extra pixels (zeros) around it.** We can pad with one or more pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-desktop",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/pd.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-football",
   "metadata": {},
   "source": [
    "There are **two common forms of padding**:\n",
    "* *Valid convolution*, which shrinks the image (no padding)  \n",
    "* *Same convolution*, where the output size is the same as input size (as in the image above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-plastic",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/pd2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-project",
   "metadata": {},
   "source": [
    "The filter size is typically odd (3x3, 5x5, 7x7 etc.) for two reasons: 1) it allows for a symmetric padding (see equation) and 2) an odd filter has a central pixel/position, which can be useful for discussing the position of a filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-pitch",
   "metadata": {},
   "source": [
    "## Strided Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-pepper",
   "metadata": {},
   "source": [
    "**In *strided convolution* we essentially \"skip\" some pixels - we move the filter multiple steps at a time.** For example, a *stride* of two moves the filter two steps at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-compression",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/sc.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-printer",
   "metadata": {},
   "source": [
    "The striding filter must reside entirely inside the image to produce an output. If it would step \"out\", the results are discarded. To respect this, we round down the output dimensions while using the formula to calculate them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-retirement",
   "metadata": {},
   "source": [
    "In maths, the convolution operation starts with a mirroring step, where the filter is flipped horizontally and vertically. In DL literature (and these lectures) we typically skip this step while still calling the operation \"convolution\". However, a more mathematically accurate name for the operation would be *cross-correlation*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-mechanics",
   "metadata": {},
   "source": [
    "## Convolutions Over Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-amendment",
   "metadata": {},
   "source": [
    "**Convolutions over volume work very similarly to 2D convolutions - we basically slide a 3D filter over a 3D image.** As before, the numbers in the filters are typically learned as parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-curtis",
   "metadata": {},
   "source": [
    "**The number of channels (depth) in the image and the filter must match.** So both would have e.g. 3 channels. The height and the width of the filter can vary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-backing",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/cov.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-exploration",
   "metadata": {},
   "source": [
    "With the filter, we can detect edges for singular channels by setting the other channels' filters to zero. Or we can detect them in all channels simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-reality",
   "metadata": {},
   "source": [
    "We can convolve with multiple filters for detecting edges (e.g. vertical and horizontal). We can then stack the results into a single \"volume\" as channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-warning",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/cov2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-ebony",
   "metadata": {},
   "source": [
    "**Convolutions over volume are useful**, because:\n",
    "1) We can operate directly with 3D images.  \n",
    "2) More importantly, we can use multiple (2 or 10 or 128 etc.) filters for detecting things and combine the outputs into a single object with multiple channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-culture",
   "metadata": {},
   "source": [
    "## One Layer of a Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-reconstruction",
   "metadata": {},
   "source": [
    "A single convolutional layer behaves as follows:  \n",
    "1) An input image is convolved with filter(s) [think: $W^{[1]}a^{[0]}$].  \n",
    "2) A (different) bias is added to the output of each filter [$z^{[1]}$] and then a non-linear function is applied to each output.  \n",
    "3) The resulting non-linear outputs are stacked into a single object [$a^{[1]}$]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-clock",
   "metadata": {},
   "source": [
    "**In a convolutional layer, the number of parameters depends on the filters - it does not depend on the size of the input image.** This is useful, since **we can process even very large images with a relatively small amount of parameters and it makes convolutional NNs less prone to overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-florist",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/olcn.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-dining",
   "metadata": {},
   "source": [
    "See below for a summary of the notation we will use in this course. This is mostly conventional, but the order of |width x depth x channels| can vary between resources and frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-center",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/olcn2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-platinum",
   "metadata": {},
   "source": [
    "## Simple Convolutional Network Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-cleanup",
   "metadata": {},
   "source": [
    "In a CNN, we often link multiple convolutional layers. **Generally, each convolutional layer decreases the weight and height compared to the original image and increases the number of channels.** We can feed the output of the last layer to a logistic/softmax neuron to make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-twelve",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/scne.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-clearing",
   "metadata": {},
   "source": [
    "A lot of the work in designing CNNs is choosing the hyperparameters: stride, padding, filter size, number of filters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-performance",
   "metadata": {},
   "source": [
    "**In CNNs, we typically use three types of layers:**  \n",
    "1) *Convolutional* (CONV)  \n",
    "2) *Pooling* (POOL)  \n",
    "3) *Fully connected* (FC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-curve",
   "metadata": {},
   "source": [
    "## Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-enemy",
   "metadata": {},
   "source": [
    "Pooling layers are used to reduce the size of the representation, speed up computation and make detecting features more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-visibility",
   "metadata": {},
   "source": [
    "**In *max pooling* we basically slide over the input a filter that takes the maximum value at each position and saves it as output.** So the pooling essentially **shrinks the input while trying to preserve any detected features (high numbers).** This has been found to work well in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-accreditation",
   "metadata": {},
   "source": [
    "**Max pooling has hyperparameters (f & s), but it has no parameters to learn.** So it is essentially a fixed computation, once the HPs have been chosen (often f=2 & s=2 or f=3 & s=3). Padding is usually not used in max pooling (p=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-nightlife",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/pl.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-porter",
   "metadata": {},
   "source": [
    "Max pooling follows the same rules we learned for the filters before - the number of channels is preserved etc. In a 3D scenario, the pooling is done separately for each channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-russell",
   "metadata": {},
   "source": [
    "**In *average pooling*, we take the average at each filter position**, instead of maximum value. This is **not used nearly as commonly as max pooling**, but it has some niche uses in collapsing certain regions in very deep NNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-pricing",
   "metadata": {},
   "source": [
    "## CNN Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-bandwidth",
   "metadata": {},
   "source": [
    "We now have all the building blocks to build a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-volleyball",
   "metadata": {},
   "source": [
    "Typically the number of layers in a NN is reported as the number of layers that have parameters. Since pooling layers have no parameters, they are not counted as separate layers. Instead, POOL layers are usually seen as follow-ups to CONV layers, since the two are commonly used together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-secretary",
   "metadata": {},
   "source": [
    "There are a lot of hyperparameters to be chosen in CNNs and the best way to choose them is usually based on literature (starting from scratch might take a long time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-plaintiff",
   "metadata": {},
   "source": [
    "A typical CNN structure is to have multiple CONV + POOL pairs and at the end a couple of FCs into a softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-humanity",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/cnne.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-forward",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/cnne2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-strengthening",
   "metadata": {},
   "source": [
    "Note, that most of the parameters are in the FCs of the network - not in the CONVs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-berry",
   "metadata": {},
   "source": [
    "## Why Convolutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-portal",
   "metadata": {},
   "source": [
    "**The main advantage of using CONV layers instead of FCs is that CONVs require way less parameters.** Especially while handling images, FCs can require crazy amounts of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-intent",
   "metadata": {},
   "source": [
    "There are **two main reasons for why CNNs have less parameters:**  \n",
    "1) *Parameter sharing*  \n",
    "  * A feature detector (e.g. vertical edges) can successfully use the same parameter values in multiple areas of the image. \n",
    "  \n",
    "2) *Sparsity of connections*\n",
    "  * In each layer, each output value only depends on a handful of inputs (not all of them, like in FCs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-tobago",
   "metadata": {},
   "source": [
    "**Since CNNs have fewer parameters, they can be trained with smaller training sets and are less prone to overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-discipline",
   "metadata": {},
   "source": [
    "CNNs are inherently good at capturing *translation invariances*. For example, they recognize that a cat picture shifted two pixels to the left is still a cat picture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-mounting",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/wc.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-burns",
   "metadata": {},
   "source": [
    "The basic principles of training CNNs are largely similar to what we learned before. We define a cost function $J$ and we optimize it with GD/Adam/etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-moore",
   "metadata": {},
   "source": [
    "In real life, CNNs are often used with the copy-paste principle. We copy a successful CNN structure that somebody else has created and use that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-necklace",
   "metadata": {},
   "source": [
    "# Week 1 - exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-somalia",
   "metadata": {},
   "source": [
    "### Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-approach",
   "metadata": {},
   "source": [
    "The **main benefits of padding** are the following:\n",
    "\n",
    "- It allows us to use a CONV layer without necessarily shrinking the height and width of the volumes. This is important for building deeper networks, since otherwise the height/width would shrink as we go to deeper layers. An important special case is the \"same\" convolution, in which the height/width is exactly preserved after one layer. \n",
    "\n",
    "- It helps us keep more of the information at the border of an image. Without padding, very few values at the next layer would be affected by pixels as the edges of an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-majority",
   "metadata": {},
   "source": [
    "In modern deep learning frameworks, you only have to implement the forward pass, and the framework takes care of the backward pass. **The backward pass for convolutional networks is complicated.**\n",
    "\n",
    "When in an earlier course we implemented a simple (fully connected) neural network, we used backpropagation to compute the derivatives with respect to the cost to update the parameters. Similarly, in convolutional neural networks we can **calculate the derivatives with respect to the cost in order to update the parameters**. The backprop equations are not trivial and we did not derive them in lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-dream",
   "metadata": {},
   "source": [
    "**Even though a pooling layer has no parameters for backprop to update, we still need to backpropagate the gradient through the pooling layer** in order to compute gradients for layers that came before the pooling layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-discipline",
   "metadata": {},
   "source": [
    "### Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-purchase",
   "metadata": {},
   "source": [
    "#### Window, kernel, filter\n",
    "The words \"window\", \"kernel\", and \"filter\" are used to refer to the same thing.  This is why the parameter `ksize` refers to \"kernel size\", and we use `(f,f)` to refer to the filter size.  Both \"kernel\" and \"filter\" refer to the \"window.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-cover",
   "metadata": {},
   "source": [
    "#### Details on softmax_cross_entropy_with_logits\n",
    "* Softmax is used to format outputs so that they can be used for classification.  It assigns a value between 0 and 1 for each category, where the sum of all prediction values (across all possible categories) equals 1.\n",
    "* Cross Entropy is compares the model's predicted classifications with the actual labels and results in a numerical value representing the \"loss\" of the model's predictions.\n",
    "* \"Logits\" are the result of multiplying the weights and adding the biases.  Logits are passed through an activation function (such as a relu), and the result is called the \"activation.\"\n",
    "* The function is named `softmax_cross_entropy_with_logits` takes logits as input (and not activations); then uses the model to predict using softmax, and then compares the predictions with the true labels using cross entropy.  These are done with a single function to optimize the calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-daughter",
   "metadata": {},
   "source": [
    "Having some trouble running stuff. Should possibly downgrade tf, if problem persists. Picked from forum: \"As I looked from coursera notebooks they are using tensorflow==1.2.1, keras == 2.0.7 these version.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-scotland",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-cooling",
   "metadata": {},
   "source": [
    "## **Case studies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-samoa",
   "metadata": {},
   "source": [
    "## Why look at case studies?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-suspension",
   "metadata": {},
   "source": [
    "This week, we'll basicly go through a bunch of examples of CNNs to see how the pros do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-bidder",
   "metadata": {},
   "source": [
    "## Classic networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-equation",
   "metadata": {},
   "source": [
    "**LeNet-5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-advertiser",
   "metadata": {},
   "source": [
    "The goal of LeNet-5 was to recognize handwritten digits. Padding wasn't used at this time. Average pooling was used, even though nowadays max would be used. The network was small, with 60 k parameters. **The height and width decrease and number of channels increases deeper into the network, as is typical in ConvNets. The basic arrangement of *conv pool conv pool fc fc output* is still quite common.** Back then, ReLU was not used - they used tanh/sigmoid. They had to do tricks with the filters due to lacking computational resources. They also had a sigmoid layer after pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-circus",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/cnln.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-field",
   "metadata": {},
   "source": [
    "**AlexNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-qatar",
   "metadata": {},
   "source": [
    "Input starts with images -> 96 filters -> max pooling -> same convolution (with padding) -> ... This net has a bunch of same conv. Uses a softmax at the end to figure which of 1000 objects the image is. AlexNet is way bigger than LeNet-5, even though the building blocks were the same - it had 60 mil parameters. Using ReLU also made this network better than LN-5. This was one of the first NNs that actually made people believe in NNs in computer vision. This paper is relatively easy to read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-theta",
   "metadata": {},
   "source": [
    "**Convolutions with same padding allow us to build very deep networks while not downsizing the input too quickly. Pooling layers are used to decrease the height and width.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-librarian",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/cnan.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-madagascar",
   "metadata": {},
   "source": [
    "**VGG-16**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-grill",
   "metadata": {},
   "source": [
    "This network really simplified the CNN architectures. Input -> same conv layer with 64 filters -> same conv layer with 64 filters -> pool -> ... This one is way deeper than the earlier two, but the structure/architecture itself is quite simple, which made it attractive to researchers. The CNN is very big even on modern standards, 138 M parameters. There is also VGG-19, which is even bigger version of this CNN, but it doesn't do that much better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-arrival",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/cnvgg.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-sentence",
   "metadata": {},
   "source": [
    "## ResNets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-harvest",
   "metadata": {},
   "source": [
    "**In a ResNet we basically jump over some layers of the NN to directly pass information deeper into the network.** ResNets are built of *residual blocks*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-origin",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/res.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-alcohol",
   "metadata": {},
   "source": [
    "**Stacking residual blocks allows us to build much deeper NNs than we could achieve with *plain networks* (non-resnets). In plain nets, the training error starts increasing after a certain number of layers, whereas in resnets, it keeps going down until it plateaus.** Using ResNets (passing info down the NN) helps with the vanishing and exploding gradients problems. The network below has 5 residual blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-pavilion",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/res2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-clark",
   "metadata": {},
   "source": [
    "## Why ResNets Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-diameter",
   "metadata": {},
   "source": [
    "ResNets can be made very deep while still doing well on the training set. Doing well on the training set is a prerequisite for doing well on the dev and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-portable",
   "metadata": {},
   "source": [
    "**Creating ResNets with many layers works well, because the layers either learn something useful or, if they don't, the NN can simply skip the useless layers by the short cuts.** So essentially, a ResNet is able to control its effective size (if we have L2-regularization or weight decay which can set terms to zero). Thus, adding more layers should rarely damage the NN, but it's possible that the extra layers learn something useful. So, we could think that for very deep NNs: ResNets $\\geq$ plain nets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-offering",
   "metadata": {},
   "source": [
    "**In ResNets we often see a bunch of same convolution, because this allows the previous activation to be summed directly to future layers (same dim. required).** If the dimensions are different, we can add the multiplier $W_s$ to match them. This can be a learned matrix or a fixed matrix implementing zero padding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-bicycle",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/wrw.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-ideal",
   "metadata": {},
   "source": [
    "## Networks in Networks and 1x1 Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-witch",
   "metadata": {},
   "source": [
    "***A network in network* or *1-by-1 convolution* allows us to control (shrink/preserve/increase) the number of channels in a network.** While a 1x1 convolution might seem trivial, it is actually useful in more than 2 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-tulsa",
   "metadata": {},
   "source": [
    "Basically we can think of 1x1 conv as a neuron, that takes in a slice of the volume, multiplies it elementwise by the weights in the 1x1 volume and takes ReLU over the results outputting a single real number. When we add multiple filters, we get multiple real numbers. So in essence, a 1x1 conv takes in some amount of numbers (32 in in image below) and outputs an amount corresponding to the number of filters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-compatibility",
   "metadata": {},
   "source": [
    "Therefore, 1x1 convolutions can be used to shrink the depth of the NN (whereas pooling layers only shrink the width and height)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-dancing",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/1by1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-spectrum",
   "metadata": {},
   "source": [
    "## Inception Network Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-element",
   "metadata": {},
   "source": [
    "**The basic idea of an *inception network* is that the network uses multiple filter sizes and/or poolings and learns which combination works best instead of us manually picking a single combination.** So, we implement multiple options and concatenate the outputs. Then, the network chooses whatever combination of filter sizes it wants by learning the parameters as it wants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-processor",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/inm.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-museum",
   "metadata": {},
   "source": [
    "**The downside of inception is the added computational cost of calculating all the options.** However, using 1x1 convolution can help us reduce the computational cost. If we 1x1 convolve to reduce the amount of channels before applying the actual (5x5 in image below) filters, we reduce the computational cost tenfold compared to directly applying the filters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-hungarian",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/inm2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-titanium",
   "metadata": {},
   "source": [
    "## Inception Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-poland",
   "metadata": {},
   "source": [
    "An *inception module/block* is a combination of filter sizes, pooling layers and 1x1 convolutions that concatenates all the output channels together. In pooling layers, the 1x1 conv comes after the pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-attachment",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/incn.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-pharmacology",
   "metadata": {},
   "source": [
    "An inception network consists of multiple inception modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-rochester",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/incn2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-testimony",
   "metadata": {},
   "source": [
    "Inception networks often have *side branches*, that try to make predictions based on hidden layers in the middle of the network. These help ensure that the intermediate features are ok at predicting the final outputs. This appears to have a regularizing effect (helps avoid overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-compilation",
   "metadata": {},
   "source": [
    "The name \"inception\" comes from the movie meme \"we need to go deeper\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-spell",
   "metadata": {},
   "source": [
    "## **Practical advice for using ConvNets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-westminster",
   "metadata": {},
   "source": [
    "## Using Open-Source Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-threat",
   "metadata": {},
   "source": [
    "**Replicating networks based on papers can be difficult. Use open-source and search for open-source implementations of existing NNs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-irish",
   "metadata": {},
   "source": [
    "When developing a computer vision project, use an existing (open-source) network architecture as a basis and use transfer learning to make it work for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-antibody",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-meditation",
   "metadata": {},
   "source": [
    "**For most computer vision applications, we should use transfer learning with an existing open-source implementation.** This will speed up the work, since it is usually much faster to use pre-trained networks than to train all the weights from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-survey",
   "metadata": {},
   "source": [
    "For example, we want to create a cat detection fNN for detecting our cats from images. We have little training data. What do?  \n",
    "1. Download an open-source network with code and weights.  \n",
    "2. Remove existing softmax output layer and replace it with our own.  \n",
    "3. Use our small training set to teach only the softmax layer (*freeze* the earlier layers).  \n",
    "4. Speed up training by \"skipping\" all the frozen layers and mapping the input *x* to the second last layer. So basically we can pre-compute the features for the second last layer and save them. Then we're basically just training a shallow softmax model based on the feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-provider",
   "metadata": {},
   "source": [
    "If we have a large dataset, we can freeze fewer layers, i.e. train more of the last layers. We can either use the existing layers and weights (as initial values) or replace the layers and reinitialize the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-following",
   "metadata": {},
   "source": [
    "If we have a LOT of data, we can retrain the entire network from scratch (replacing softmax output to fit our needs). In this case, we could still use the pretrained weights as initial values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-reliance",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-arlington",
   "metadata": {},
   "source": [
    "**Most existing computer vision applications would like to have more data to improve the model (this is not true for all DL fields). Data augmentation is a way to produce more data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-pasta",
   "metadata": {},
   "source": [
    "Common data augmentation methods for images:  \n",
    "* Mirroring  \n",
    "* Random cropping\n",
    "* Color shifting (e.g. randomly add 5 to RGB channels)\n",
    "\n",
    "Less commonly used methods, but still worth a try:  \n",
    "* Rotation  \n",
    "* Shearing  \n",
    "* Local warping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-nurse",
   "metadata": {},
   "source": [
    "Color shifting should be done using PCA (principal component analysis). For example, for an image with a lot of RB and little G, PCA might reduce RB values by a lot and reduce G just slightly to keep the overall balance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-thunder",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/da.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-deposit",
   "metadata": {},
   "source": [
    "Augmentation might require a lot of disk space, if all the images are saved. Consequently, the augmentation is usually done on the fly with e.g. a single CPU/GPU thread that augments images to create mini-batches of data. This data is then passed forward to the rest of the CPU/GPU that does the training. These processes can happen in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-glenn",
   "metadata": {},
   "source": [
    "**Data augmentation also has some hyperparameters, so it can be a good idea to look for existing open-source implementations of the augmentation methods.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-walnut",
   "metadata": {},
   "source": [
    "## State of Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-melissa",
   "metadata": {},
   "source": [
    "There are some things that are unique about deep learning in computer vision as opposed to other fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-guinea",
   "metadata": {},
   "source": [
    "Even though there is a lot of data for *image recognition* (\"is there a cat in the picture\") online, it also requires a LOT of data and could always use more. Some problems like *object recognition* (\"where is the cat in the picture\") have relatively very little data in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-sentence",
   "metadata": {},
   "source": [
    "**When you have large amounts of data, you can use simpler algorithms and less hand-engineering. When you have little data, you will need more hand-engineering (\"hacks\").**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-platform",
   "metadata": {},
   "source": [
    "A learning algorithm has two sources of data:  \n",
    "1. Labeled data  \n",
    "2. Hand-engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-indonesian",
   "metadata": {},
   "source": [
    "**Computer vision relies quite a lot on hand-engineering, since historically there has always been too little data.** The amount of data is increasing now, but some hand-engineering is still going on. Nowadays, transfer learning also helps with small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-grass",
   "metadata": {},
   "source": [
    "Hand-engineering is not bad per se, it takes a lot of skill and effort to work with small amounts of data. But if there's a lot of data, it's not really required and we can focus on other parts of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-warehouse",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/socv.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-orbit",
   "metadata": {},
   "source": [
    "**In research, people are very fixated on doing well on standardized benchmark datasets and winning competitions, because this can help publish a paper.** The upside of this is, that it helps the entire community figure out the most effective methods. The downside is, that to do well on benchmarks researches might apply tricks/hacks that would never be used in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-attack",
   "metadata": {},
   "source": [
    "**Some tricks, that do well on benchmarks but are not used for production systems:**\n",
    "* Ensembling - train several NNs independently and average the outputs $\\hat{y}$ (max. 1-2 % improvement). This slows down runtime considerably, since we need to run every image through multiple (often 3 to 15) networks. Also, we need to keep all the networks available, which sucks memory.  \n",
    "* Multi-crop at test time - augment the test images before running them through the network and average the results over the augmented images. A typical method is *10-crop*. While resource-heavy, at least it only requires one network as opposed to ensembling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-mailing",
   "metadata": {},
   "source": [
    "We should take advantage of open-source code and available research papers in our projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-large",
   "metadata": {},
   "source": [
    "# Week 2 - exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-therapy",
   "metadata": {},
   "source": [
    "## Keras tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-causing",
   "metadata": {},
   "source": [
    "### Key Points to remember\n",
    "- Keras is a tool we recommend for rapid prototyping. It allows you to quickly try out different model architectures.\n",
    "- Remember The four steps in Keras: \n",
    "\n",
    "\n",
    "1. Create  \n",
    "2. Compile  \n",
    "3. Fit/Train  \n",
    "4. Evaluate/Test  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-marketplace",
   "metadata": {},
   "source": [
    "## Residual Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-territory",
   "metadata": {},
   "source": [
    "### What you should remember\n",
    "- Very deep \"plain\" networks don't work in practice because they are hard to train due to vanishing gradients.  \n",
    "- The skip-connections help to address the Vanishing Gradient problem. They also make it easy for a ResNet block to learn an identity function. \n",
    "- There are two main types of blocks: The identity block and the convolutional block. \n",
    "- Very deep Residual Networks are built by stacking these blocks together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-eating",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-polish",
   "metadata": {},
   "source": [
    "## Detection algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-explorer",
   "metadata": {},
   "source": [
    "### Object Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-sally",
   "metadata": {},
   "source": [
    "**Localization refers to figuring out where in the picture is the object to be detected.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-disclosure",
   "metadata": {},
   "source": [
    "In *image classification* with/without *localization* there is only one object in the image. In *detection*, there can be multiple of different types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-elite",
   "metadata": {},
   "source": [
    "Image classification can be done with a ConvNet as we learned before. **For classification with localization we can add four new output neurons for the bounding box coordinates, width and height.** We typically use a single output for dictating if there is an object and the others to describe it, should it exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-august",
   "metadata": {},
   "source": [
    "In classification with localization, the loss function usually has log-likelihood for the object labels $c$, squared error for bounding box coordinates, logistic regression loss for $p_c$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-chair",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/ol2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-scholar",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/ol.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-commerce",
   "metadata": {},
   "source": [
    "### Landmark Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-blocking",
   "metadata": {},
   "source": [
    "We can have a NN output the x and y coordinates of **important points in an image, aka *landmarks***, that we want the NN to recognize. We can have the landmark coordinates in the NN outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-concrete",
   "metadata": {},
   "source": [
    "**The landmarked object can be for example a feature in a persons face (corners of eyes, nose, mouth).** This is a basic building block for recognizing emotions from faces, for snapchat face filters etc. Another example is pose detection, where we identify landmarks for different body parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-chase",
   "metadata": {},
   "source": [
    "Training a NN with landmarks requires training data with labeled landmarks. The landmarks must be consistent across all image, e.g. the coordinates $l_1$ are always right corner of left eye."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-airfare",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/ld.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-kernel",
   "metadata": {},
   "source": [
    "### Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-sport",
   "metadata": {},
   "source": [
    "Object localization and landmark detection build up to *object detection*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-security",
   "metadata": {},
   "source": [
    "Using car detection as an example: First, we get a training set with very closely cropped images of cars. Then, we use the images to train a ConvNet to recognize if there is a car in a given image. The trained ConvNet can then be used in *sliding windows detection*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-sponsorship",
   "metadata": {},
   "source": [
    "**In sliding windows detection, we slide a boundary box over the image, taking snapshots at each location and passing them to the CNN, that detects if there is a car in the box.** The box/window is slid over the entire image with a chosen stride. Then, we choose a bigger window and slide it over the image. Then, we can choose an even bigger window. The aim is to find crops where the CNN can recognize cars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-nature",
   "metadata": {},
   "source": [
    "**The computational cost of sliding windows detection is great due to the amount of crops that we have to pass through the CNN.** Using a bigger stride will decrease computational cost and decrease accuracy and a smaller one will increase both (up to a point)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-hamburg",
   "metadata": {},
   "source": [
    "Sliding window detection was a fine method in the past, where the models were simpler and lighter. However, **its standard form is too slow and/or inaccurate for huge modern NNs**. Luckily, there is a modern way to implement SWD more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-architecture",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/od.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-instrumentation",
   "metadata": {},
   "source": [
    "### Convolutional Implementation of Sliding Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-scott",
   "metadata": {},
   "source": [
    "**We can implement sliding windows convolutionally to make the method efficient by modern standards.** Normally, separate sliding windows do separate and overlapping computations, which \"wastes\" a lot of resources. However, in a convolutional implementation, we can share some of the computations between the windows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-apartment",
   "metadata": {},
   "source": [
    "The first step is **turning fully connected layers into convolutional layers by computing the FC layers as a 3-D volumes, \"1-by-1-by-$z$\" layers**, through convolution with filters. The output layer is created in a similar way - as multiple values corresponding to the class probabilities of each class the softmax is identifying."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-campbell",
   "metadata": {},
   "source": [
    "Then, **instead of running the CNN forward prop multiple times for different crops of the image, we can run it once on the entire image and get an output describing all of the individual crops.** This significantly lowers the computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-wealth",
   "metadata": {},
   "source": [
    "However, the positions of the bounding box are not very accurate with this implementation. Next, we'll learn how to fix this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-convert",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/cisw.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-census",
   "metadata": {},
   "source": [
    "### Bounding Box Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-figure",
   "metadata": {},
   "source": [
    "Sometimes none of the sliding window positions match perfectly with the object to be detected. And sometimes the bounding box is not really a box/square, it can have different aspect ratios. Luckily, **we can use the *YOLO algorithm* to get more accurate bounding boxes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-acceptance",
   "metadata": {},
   "source": [
    "YOLO works as follows:  \n",
    "1) Set a grid (in example 3x3, typically 19x19) over the input image (in example 100x100).  \n",
    "2) Apply the image classification and localization algorithm to each grid cell.  \n",
    "3) **Assign each object to a grid cell based on the midpoint of the object**\\.  \n",
    "4) The target output is a 3x3x8 volume (grid x grid x labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-burst",
   "metadata": {},
   "source": [
    "**YOLO is usually done with a CNN, which maps the image to the output volume.** This makes the algorithm very efficient, since we are just running an image through the CNN once - it's a convolutional implementation. YOLO is so fast it even works for real-time object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-rally",
   "metadata": {},
   "source": [
    "The advantage of YOLO is that it gives precise bounding boxes. The disadvantage is that there can only be one object per grid cell (in the default implementation), however with a finer grid like 19x19 it's less likely that there will be multiple objects in a single cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-rebound",
   "metadata": {},
   "source": [
    "In YOLO, the top left point of each grid cell has coordinates (0,0) and the lower right (1,1). The object midpoint coordinates are specified based on these coordinates. The height of the bounding box is specified as a fraction of the total grid cell height, and similarly for width."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-editing",
   "metadata": {},
   "source": [
    "While the midpoint coordinates only get values within [0,1], the width and he height can be > 1 if the object is bigger than the bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-yukon",
   "metadata": {},
   "source": [
    "**In the YOLO algorithm, at training time, only one cell ---the one containing the center/midpoint of an object--- is responsible for detecting this object.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-latex",
   "metadata": {},
   "source": [
    "There are other parametrizations that might work better in the original YOLO paper. It's quite a hard read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-restriction",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/bbp.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-scheme",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/bbp2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-tamil",
   "metadata": {},
   "source": [
    "### Intersection Over Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-device",
   "metadata": {},
   "source": [
    "We can use *the intersection of union* function to tell if our object detection algorithm is working well. We can also include it as a component in our OD algorithm to make it work better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-arthur",
   "metadata": {},
   "source": [
    "**IoU basically takes two bounding boxes and divides the size of their intersection by the size of their union. The standard value in computer vision tasks accepted as clear overlap is 0.5** - however stricter values such as 0.6 or 0.7 can also be used based on the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-malpractice",
   "metadata": {},
   "source": [
    "Basically, we can compare the predicted box with the true box to tell if our OD is working well. However, IoU generally measures how much any two bounding boxes overlap and can be used to improve our OD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-synthetic",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/iou.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-graduate",
   "metadata": {},
   "source": [
    "### Non-max Suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-kitty",
   "metadata": {},
   "source": [
    "Sometimes, our object detection algorithm might detect the same object multiple times. **We can use non-max suppression to make sure each object is only detected once.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-north",
   "metadata": {},
   "source": [
    "It's possible that multiple YOLO grid cells think they found the center point of the object. This can lead to multiple detections of each object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-intersection",
   "metadata": {},
   "source": [
    "**Non-max suppression compares the probabilities of each object detection.** It will take the highest probability as the \"true\" detection and suppress any lower-probability detection boxes that have a significant overlap with the true box. Then, it checks any remaining non-supressed detections and picks the one with the highest probability, repeating the process. **In the end, we are left with the unique highest probability boxes, which are the final predictions.** (In reality, the process is slightly more complex but this is the general idea)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-developer",
   "metadata": {},
   "source": [
    "The algorithm can also be written as follows:  \n",
    "1) Cut out any bounding boxes with probabilites lower than a certain threshold (e.g. < 0.6).  \n",
    "2) Choose the box with the maximum probability.  \n",
    "3) Discard any box that has IoU > 0.5 with the maximum box.  \n",
    "4) Repeat 2 and 3 until no new boxes remain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-tiger",
   "metadata": {},
   "source": [
    "**If we have multiple output classes, we carry out NMS separately for each of the classes.** Details in the programming exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-nursery",
   "metadata": {},
   "source": [
    "The algorithm supresses any probability values that are not individual maxima -> non-max suppression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-lightning",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/nms.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-curve",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/nms2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-columbus",
   "metadata": {},
   "source": [
    "### Anchor Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-climate",
   "metadata": {},
   "source": [
    "One problem we saw with OD so far is that each grid cell can only detect a single object. **Using *anchor boxes*, we can have a grid cell detect multiple objects.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-allen",
   "metadata": {},
   "source": [
    "We predefine a number of anchor boxes (two in lecture example) and reflect this by adding additional outputs $y$ corresponding to each box. Then, we can associate each object with an anchor box of similar shape using IoU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-factor",
   "metadata": {},
   "source": [
    "Essentially, **each object is assigned to a grid cell *and* an anchor box.** This allows for detection of multiple objects within the same grid cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-annual",
   "metadata": {},
   "source": [
    "Anchor box limitations:  \n",
    "* We **need to have as many anchor boxes as we expect to find objects in a single grid cell.** So if we have 2 anchor boxes but find 3 objects in a single cell, the algorithm will not work properly, unless we implement some sort of tie breaker.\n",
    "* Anchor boxes fail, if we have multiple objects (of similar shape) that can be associated with the same anchor box. This case would also require a tie breaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-sitting",
   "metadata": {},
   "source": [
    "**In practice, two objects having the same midpoint in a 19x19 grid is quite rare.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-tyler",
   "metadata": {},
   "source": [
    "Anchor boxes allow the learning algorithm to specialize better - some neurons can specialize in wide objects and others in tall objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-lighting",
   "metadata": {},
   "source": [
    "**Anchor boxes need to be chosen by the modeller.** Before, they were chosen purely by hand to reasonably represent the object shapes that were expected. Nowadays, a more advanced approach would be using a k-means algorithm to group together the shapes that the objects tend to get and use this to choose the most representative anchor boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-watts",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/ab.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-newcastle",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/ab2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-concrete",
   "metadata": {},
   "source": [
    "### YOLO Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-physics",
   "metadata": {},
   "source": [
    "Here, we put all the above pieces together to form a practical YOLO algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-military",
   "metadata": {},
   "source": [
    "**Training**. We train a CNN that inputs a 100x100x3 image and outputs a 3x3x16 volume. Each grid cell is gone through separetely and considered for all the anchor boxes. An object is detected for a grid cell and anchor box combination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-recall",
   "metadata": {},
   "source": [
    "**Predicting**. There are numerical values even in cells with no objects, but these are trash."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-jerusalem",
   "metadata": {},
   "source": [
    "**Non-max suppression**. Due to having two bounding boxes, we get two bounding box predictions for each grid cell. Some of the boxes have very low probabilities. We do NMS to generate final predictions for each class we try to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-click",
   "metadata": {},
   "source": [
    "**YOLO is one of the most effective OD algorithms.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-easter",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/yolo.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-command",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/yolo2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-alabama",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/yolo3.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-partner",
   "metadata": {},
   "source": [
    "### Region Proposals (optional lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-herald",
   "metadata": {},
   "source": [
    "Region proposal algorithms are not used very commonly nowadays, but it's still good to know what they are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-ribbon",
   "metadata": {},
   "source": [
    "Basically, RP selects a few meaningful windows and runs the classifier on those, instead of sliding a window over the whole image. This is based on a segmentation algorithm, which tries to find regions of importance. This is the basic R-CNN (regions with CNN) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-reminder",
   "metadata": {},
   "source": [
    "R-CNN is slow, but there are faster modern alternatives. For example, *Fast R-CNN* is a convolutional implementation. *Faster R-CNN* does the region proposals with a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-miami",
   "metadata": {},
   "source": [
    "Even the modern CNN algorithms are usually slower than YOLO. According to Ng, YOLO is a more promising algorithm and RP is more \"nice-to-know\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-shark",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/rp.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-jonathan",
   "metadata": {},
   "source": [
    "# Week 3 - exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-wellington",
   "metadata": {},
   "source": [
    "## What we should remember:\n",
    "    \n",
    "- YOLO is a state-of-the-art object detection model that is fast and accurate\n",
    "- It runs an input image through a CNN which outputs a 19x19x5x85 dimensional volume. \n",
    "- The encoding can be seen as a grid where each of the 19x19 cells contains information about 5 boxes.\n",
    "- You filter through all the boxes using non-max suppression. Specifically: \n",
    "    - Score thresholding on the probability of detecting a class to npeep only accurate (high probability) boxes\n",
    "    - Intersection over Union (IoU) thresholding to eliminate overlapping boxes\n",
    "- Because training a YOLO model from randomly initialized weights is non-trivial and requires a large dataset as well as lot of computation, we used previously trained model parameters in this exercise. If you wish, you can also try fine-tuning the YOLO model with your own dataset, though this would be a fairly non-trivial exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-acting",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-trace",
   "metadata": {},
   "source": [
    "## Face Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-zealand",
   "metadata": {},
   "source": [
    "### What is face recognition?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-probability",
   "metadata": {},
   "source": [
    "Face recognition is a special application of CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-voice",
   "metadata": {},
   "source": [
    "***Face verification* and *face recognition* are two different problems. Recognition is more difficult (requires accuracy > 99 %).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-slide",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/wfr.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-desert",
   "metadata": {},
   "source": [
    "### One Shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-adrian",
   "metadata": {},
   "source": [
    "In face recognition we need to solve the ***one shot learning problem*. This means, that our system needs to recognize a face based on a single photo (there is just one photo of each employee in the database).** This is traditionally a difficult task for NNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-franklin",
   "metadata": {},
   "source": [
    "A bad approach would be training a CNN into a softmax with outputs corresponding to different faces. This does not work well, since the training set is too small to train the NN properly. Also, adding new personnel would be difficult and require retraining the NN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-relief",
   "metadata": {},
   "source": [
    "Instead, we will learn **a *similarity* function. This compares two images and returns a value describing if they are of the same person (low value) or not (high).** We can then set a threshold and if the returned value is under the threshold, then the two images are of the same person. -> Verification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-theater",
   "metadata": {},
   "source": [
    "**In a recognition task, we apply the similarity function to the photo and each face in the database.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-board",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/osl.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-split",
   "metadata": {},
   "source": [
    "### Siamese Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-might",
   "metadata": {},
   "source": [
    "**In a *siamese network*, we pass two images through the same CNN and** compare the outputs. The output layer is typically fully connected, and it gives a representation (\"an encoding\") of the full image. We can then **compare the two images' encodings and use this comparison as the similarity function - to decide if the images portray the same person.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-intermediate",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/sn.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-aurora",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/sn2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-organ",
   "metadata": {},
   "source": [
    "### Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-ivory",
   "metadata": {},
   "source": [
    "**The *triplet loss function* is defined on triplets of images: an anchor (A, target image), a positive (P, similar to anchor) and a negative (N, different to anchor).** This loss function is used to train the NN to differentiate between similar and dissimilar images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-cameroon",
   "metadata": {},
   "source": [
    "We want the encodings of A - P to be less or equal to the encodings of A - N. Furthermore, we add a *margin* hyperparameter to the equation. The margin pushes the AP and the AN pairs further away from each other, making it harder to satisfy the comparison. The margin also prevents the NN from outputting trivial solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-uganda",
   "metadata": {},
   "source": [
    "**We cannot choose the triplets totally randomly - we should choose triplets that are \"difficult\" to train on (difference is small).** If we choose them randomly, the task will be too easy for gradient descent and the NN won't work properly. Furthermore, **for training we require multiple pictures of each person.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-biology",
   "metadata": {},
   "source": [
    "In the DL world, algorithms are often named \"\\_\\_\\_Net\" or \"Deep\\_\\_\\_\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-tribute",
   "metadata": {},
   "source": [
    "**Typically for facial recognition, we would use a NN trained by somebody else instead of implementing it from scratch.** Big companies are training such networks with tens or hundreds of millions of images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-formula",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/tl.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-cooler",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/tl2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-fortune",
   "metadata": {},
   "source": [
    "### Face Verification and Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-dakota",
   "metadata": {},
   "source": [
    "We learned that triplet loss is one way to approach face recognition. However, face recognition can also be posed as a *binary classification problem*. It also works well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-commission",
   "metadata": {},
   "source": [
    "In binary classification, we give the siamese network FC output to a logistic regression unit, which decides whether the images are of the same (1) or different (0) person."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-sauce",
   "metadata": {},
   "source": [
    "We can precompute the encodings for the database images to save computational resources. So, we only compute the encodings for the new images taken of employees trying to enter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-indonesia",
   "metadata": {},
   "source": [
    "The training set consists of pairs of images labeled 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-demonstration",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/fvbc.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-carter",
   "metadata": {},
   "source": [
    "## Neural Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-pontiac",
   "metadata": {},
   "source": [
    "### What is neural style transfer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-peace",
   "metadata": {},
   "source": [
    "***Neural style transfer* = applying the style of an image (S) to content image (C) resulting in a generated image (G).** For example, making a photo have the style of some old painting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-marketing",
   "metadata": {},
   "source": [
    "This is based on using the features in the intermediate layers of a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-still",
   "metadata": {},
   "source": [
    "### What are deep ConvNets learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-central",
   "metadata": {},
   "source": [
    "Neurons in the early CNN layers learn simple features like edges or certain shades of color."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-peeing",
   "metadata": {},
   "source": [
    "Neurons in deeper layers recognize more complex features/shapes. It can be hard to say for sure what exactly each neuron is detecting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-applicant",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/wcl.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-jumping",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-pasta",
   "metadata": {},
   "source": [
    "The cost function measures how good a particular generated image is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-watson",
   "metadata": {},
   "source": [
    "Two parts:  \n",
    "* content cost $J$ measures how similar the content of C and G are.\n",
    "* style cost $J$ measures how similar the style of S and G are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-egypt",
   "metadata": {},
   "source": [
    "So, the cost is of the form:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-gauge",
   "metadata": {},
   "source": [
    "$J(G) = \\alpha J_{content}(C,G) + \\beta J_{style}(S,G)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-canon",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/cf.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-edmonton",
   "metadata": {},
   "source": [
    "### Content Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-theory",
   "metadata": {},
   "source": [
    "**We choose a hidden layer $l$ for computing the content cost. The hidden layer should be somewhere in the middle of the CNN** - not too deep and not too early."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-memphis",
   "metadata": {},
   "source": [
    "The *content cost function* is essentially just the element-wise sum of squares of differences between the activations in layer $l$\n",
    "of the images C and G."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-cover",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/ccf.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-plumbing",
   "metadata": {},
   "source": [
    "### Style Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-backing",
   "metadata": {},
   "source": [
    "**The \"style\" of an image is defined as the correlation between activations across channels in layer *l*.** For example, we take the first and second channel and compare them value by value checking how correlated the values are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-fifteen",
   "metadata": {},
   "source": [
    "The correlation tells how often the high-level features (e.g. texture, color) occur or do not occur together. For example, if channel 1 is orange every time channel 2 has vertical lines, there is high correlation between these features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-holocaust",
   "metadata": {},
   "source": [
    "**For the cost function, we calculate a style matrix *G* (aka \"gram matrix\"), that computes all the required correlations and thus tells us how correlated the values are between channels *k* and *k'*.** To form *G*, we sum over the image height and width and multiply together the activations of the channels *k* and *k'*. We do this for every value of *k* and *k'*. If *G* has high values, there is a high correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-encyclopedia",
   "metadata": {},
   "source": [
    "Technically, **in *G* we do not compute correlation but *unnormalized cross-covariance*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-utility",
   "metadata": {},
   "source": [
    "The style matrix ***G* is computed for both the style image S and generated image G. *The style cost function J* is computed as the sum of squares of the elementwise differences between the style matrices, summed over all the layers.** Computing *J* over all the layers allows us to include both lower and higher level features in the \"style\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-entity",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/scf.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-tiffany",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/scf2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-entertainment",
   "metadata": {},
   "source": [
    "### 1D and 3D Generalizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-arrangement",
   "metadata": {},
   "source": [
    "We can apply to CNNs to more than just 2D images - they also work for 1D and 3D data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-marshall",
   "metadata": {},
   "source": [
    "For 1D data, we convolve a 1D input with a number of (e.g. 16 & 32 in slides) 1D filters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-patrol",
   "metadata": {},
   "source": [
    "Similarly, for 3D data we convolve a 3D input with a number of 3D filters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-arbor",
   "metadata": {},
   "source": [
    "CNNs are mostly used on 2D images, since image data are so pervasive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-hands",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/1d3d.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-balloon",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/1d3d2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-communist",
   "metadata": {},
   "source": [
    "# Week 4 - exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-nutrition",
   "metadata": {},
   "source": [
    "## Art generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-popularity",
   "metadata": {},
   "source": [
    "In this model the optimization algorithm updates the pixel values rather than the neural network's parameters. Deep learning has many different types of models and this is only one of them! \n",
    "\n",
    "\n",
    "#### Key points to remember\n",
    "**General**  \n",
    "- Neural Style Transfer is an algorithm that given a content image C and a style image S can generate an artistic image\n",
    "- It uses representations (hidden layer activations) based on a pretrained ConvNet.  \n",
    "- We get even better results by combining this representation from multiple different layers. \n",
    "- This is in contrast to the content representation, where usually using just a single hidden layer is sufficient.\n",
    "\n",
    "**Content cost**\n",
    "- The content cost function is computed using one hidden layer's activations.\n",
    "- The content cost takes a hidden layer activation of the neural network, and measures how different $a^{(C)}$ and $a^{(G)}$ are.\n",
    "\n",
    "**Style cost**  \n",
    "- The style of an image can be represented using the Gram matrix of a hidden layer's activations. \n",
    "- The style cost function for one layer is computed using the Gram matrix of that layer's activations. The overall style cost function is obtained using several hidden layers.\n",
    "- Minimizing the style cost will cause the image $G$ to follow the style of the image $S$.\n",
    "\n",
    "**Total cost**\n",
    "- The total cost is a linear combination of the content cost $J_{content}(C,G)$ and the style cost $J_{style}(S,G)$.\n",
    "- Optimizing the total cost function results in synthesizing new images.\n",
    "- $\\alpha$ and $\\beta$ are hyperparameters that control the relative weighting between content and style.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-signal",
   "metadata": {},
   "source": [
    "## Face recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-grant",
   "metadata": {},
   "source": [
    "#### Key points to remember\n",
    "- Face verification solves an easier 1:1 matching problem; face recognition addresses a harder 1:K matching problem. \n",
    "- The triplet loss is an effective loss function for training a neural network to learn an encoding of a face image.\n",
    "- The same encoding can be used for verification and recognition. Measuring distances between two images' encodings allows you to determine whether they are pictures of the same person. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

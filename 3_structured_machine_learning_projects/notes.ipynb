{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "parliamentary-motion",
   "metadata": {},
   "source": [
    "# Structuring machine learning projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-typing",
   "metadata": {},
   "source": [
    "## Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-investment",
   "metadata": {},
   "source": [
    "### **Introduction to ML strategy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-wagner",
   "metadata": {},
   "source": [
    "### Why ML strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-appendix",
   "metadata": {},
   "source": [
    "While making a model, we might have a huge amount of ideas that might improve the model. However, it's easy to get lost in the chaos or follow false leads so structuring the ideas is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-adobe",
   "metadata": {},
   "source": [
    "This course aims to provide tools to analyze the ML problem at hand and to find the best solution to solve/improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-jurisdiction",
   "metadata": {},
   "source": [
    "The course is very much based on practical experience of Ng and contains ideas that might not see as much light in university lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-desktop",
   "metadata": {},
   "source": [
    "### Orthogonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-preservation",
   "metadata": {},
   "source": [
    "Orthogonalization through examples:  \n",
    "* An old tv, which has separate knobs for controlling picture width, height, shape etc. Each knob has exactly one function, instead of each of the knobs changing all the properties slightly.  \n",
    "* A car has a wheel for steering and peddles for controlling speed. So each part of the car has a separate function. Imagine instead, that by turning the wheel right the car speeds up and steers right and by turning the wheel left the car slows down and steers left. This would be much harder to drive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-shepherd",
   "metadata": {},
   "source": [
    "Orthogonalization as a concept basically means that you want a separate \"knob\" to control each individual feature. You do not want to mix multiple things together behind a single knob, because that becomes very hard to control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-vietnamese",
   "metadata": {},
   "source": [
    "Orthogonalization comes from orthogonal. Think of axes that are orthogonal to each other. You want to move the \"slider\" along a single axis, not somewhere between multiple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-shopping",
   "metadata": {},
   "source": [
    "So basically you want to have a clear toolset to achieve each individual improvement. For machine learning this means that you have a specific set of tricks to try when you want to a) make the model fit the training set better, b) make the model to fit the dev set better etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-supplier",
   "metadata": {},
   "source": [
    "For example, early stopping is a method that is less orthogonalized since it simultaneously affects a) and b). This makes it more difficult to use successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-gabriel",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/ortho.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-technique",
   "metadata": {},
   "source": [
    "### **Setting up your goal**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-bracelet",
   "metadata": {},
   "source": [
    "### Single number evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-trout",
   "metadata": {},
   "source": [
    "When we are trying new model types, we want to quickly see what is the best model, so we can keep iterating and making it better. Therefore, it's handy to choose a single metric, a single real number, to describe how well the model is doing on the dev set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-kingston",
   "metadata": {},
   "source": [
    "**The *F1-score* is a standard metric used in literature. It combines (harmonic mean) the metrics *precision* and *recall* into a single number**. Calculating *F1-score* with a good dev set allows us to see with a single glance which model type is the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-procedure",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/F1_score.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-dealing",
   "metadata": {},
   "source": [
    "If we have multiple F1-scores in multi-class classification (cat recognizion accuracy in Finland, India, China, Namibia, other), you can average over the scores to quickly see which model type did the best at all of the sites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-marketing",
   "metadata": {},
   "source": [
    "### Satisficing and Optimizing metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-survival",
   "metadata": {},
   "source": [
    "Sometimes it's not possible to include all the information for choosing the model in a single real number - maybe we care about both accuracy (e.g. F1) and running time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-presentation",
   "metadata": {},
   "source": [
    "If there are multiple metrics we care about, such as accuracy and running time, we should choose one metric as an *optimizing metric* and (some of) the others as *satisficing metrics*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-sterling",
   "metadata": {},
   "source": [
    "**The optimizing metric (e.g. accuracy) is something that we want to be as good as possible within the limits of the satisficing metric(s) (e.g. running time, # of false positives)**. For example, we might want the accuracy to be as good as possible while having a running time of less than 100 ms. We would choose the model that best satisfies these conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-sussex",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/sat&opt.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-reputation",
   "metadata": {},
   "source": [
    "### Train/dev/test distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-package",
   "metadata": {},
   "source": [
    "It's important to properly set up the train set, dev (\"development\" or \"hold out cross validation\") set and test set to keep the workflow efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-incidence",
   "metadata": {},
   "source": [
    "You should always take your dev and test sets from the same distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-hacker",
   "metadata": {},
   "source": [
    "So for example, do NOT choose a dev set with cat pictures from the Nordic countries and a test set with cat pictures from Southern Africa. This way you might spend months optimizing the model on the dev set only to find out it works horribly with the test set. Instead, take all the cat pictures and randomly shuffle them into a dev and test set so both sets have Nordic and Southern cat pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-disposal",
   "metadata": {},
   "source": [
    "In a nutshell: **Choose a dev set and test set (from the same distribution) to reflect data you expect to get in the future and consider important to do well on.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-aviation",
   "metadata": {},
   "source": [
    "We will talk about the training set later. The training set basically defines how well we are able to optimize the model to the dev set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-diary",
   "metadata": {},
   "source": [
    "### Size of the dev and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-johns",
   "metadata": {},
   "source": [
    "Back in the day a 70-30 % train-test split was common. Or a 60-20-20 % train-dev-test split. These were reasonable splits back when the data sets were small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-median",
   "metadata": {},
   "source": [
    "However, deep learning is very data hungry so it's beneficial to have as much data for training as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-copying",
   "metadata": {},
   "source": [
    "**Nowadays, if you have 1,000,000 examples, it can be reasonable to do a 98-1-1 % split. This still yields 10,000 examples to both dev and test sets, which might already be enough.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-lease",
   "metadata": {},
   "source": [
    "The purpose of the test set is to help us understand how well our model might actually do in a real-world scenario. Consequently, the test set should be big enough to give us a high confidence in the overall performance of the model. This size can be 10,000 or 100,000 or different depending on the application. Sometimes people do not use a test set (only train and dev sets) but that's not really recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-asbestos",
   "metadata": {},
   "source": [
    "### When to change dev/test sets and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-architecture",
   "metadata": {},
   "source": [
    "Sometimes we might realize during the ML process that we didn't define the dev/test sets or metric properly and need to change them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-vacuum",
   "metadata": {},
   "source": [
    "The evaluation metric we chose might not work as intended: e.g. it might have the highest accuracy but also let through porn images. In this case, we need to revise the metric and artificially give the porn images a higher error. This will involve changing the metric and possibly going through the dev/test sets to label porn images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-municipality",
   "metadata": {},
   "source": [
    "Another case might be that your dev/test sets contain only perfect images, while the users will also submit blurry ones. In this case, you might need to change your dev/test sets by including more amateurish images and try to do better on them by tweaking your metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-practice",
   "metadata": {},
   "source": [
    "Here we have an example of orthogonalization, in two separate steps:  \n",
    "1) Place the target, choose what metric we want to optimize.  \n",
    "2) Figure out how to hit the target as accurately as possible, e.g. change the cost equation to do well on the chosen metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-external",
   "metadata": {},
   "source": [
    "**So, try to think of a good metric and good dev/test set when starting out to make the ML process go quicker. But, if you cannot immediately come up with a \"perfect\" metric, start with something and change it once you realize it's not working.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-isolation",
   "metadata": {},
   "source": [
    "### **Comparing to human-level performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-graduation",
   "metadata": {},
   "source": [
    "### Why human-level performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-travel",
   "metadata": {},
   "source": [
    "Why do we want to compare ML systems to human-level performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-visibility",
   "metadata": {},
   "source": [
    "1) It's actually feasible to reach human-level performance with modern deep learning algorithms.  \n",
    "2) The workflow is more efficient when we try to solve a problem humans could also do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-homework",
   "metadata": {},
   "source": [
    "**Typically ML model accuracy progresses fast until it surpasses human-level performance. After this it slows down. The maximum accuracy that the model can have is called the *Bayes (optimal) error*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-estate",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/hlp.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-olive",
   "metadata": {},
   "source": [
    "The progress slows down, because:  \n",
    "1) **The human-level performance is close to Bayes error for many tasks.**  \n",
    "2) It is easier to train a model up to human-level performance than past it. As long as the ML algorithm is worse than humans, we can:  \n",
    "  * Get labeled data from humans.\n",
    "  * Do manual error analysis, why did a person get it right if the algorithm got it wrong.  \n",
    "  * Get a better analysis of bias/variance (following lectures)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-laptop",
   "metadata": {},
   "source": [
    "### Avoidable bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-poker",
   "metadata": {},
   "source": [
    "Knowing how well humans do with a certain problem, e.g. classification, we can understand how well we want the ML algorithm to do on the training set. This is based on point 1) above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-anthony",
   "metadata": {},
   "source": [
    "For example, **if humans can recognize cat images in the training set with an accuracy of 1 %, a ML algorithm with 8 % is not great and is likely underfitting (high bias). If the human accuracy is 7,5 %, a ML algorithm with 8 % is doing great and we can focus on reducing overfitting (high variance).** So basically, knowing the human-level error can help us understand if we should focus on bias or variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-dutch",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/abias.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-variance",
   "metadata": {},
   "source": [
    "### Understanding human-level performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-bosnia",
   "metadata": {},
   "source": [
    "**The \"human-level error\" can have different definitions but often it is useful to define it as the best result humans could possibly obtain. This way, it can be used to approximate Bayes error as accurately as possible.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-spank",
   "metadata": {},
   "source": [
    "### Surpassing human-level performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-deployment",
   "metadata": {},
   "source": [
    "Once the ML model surpasses human-level performance, it can be hard to deduce the Bayes error and hence understand if we are dealing with a bias or variance problem. Furthermore, at that point we can no longer use human-labeled data to efficiently teach the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-modification",
   "metadata": {},
   "source": [
    "Some problems, where ML significantly surpasses human-level performance:  \n",
    "* Online advertising  \n",
    "* Product recommendations  \n",
    "* Logistics (predicting transit time)  \n",
    "* Loan approvals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-moderator",
   "metadata": {},
   "source": [
    "All of the above problems rely on structured data (big databases of tables). They are not natural perception problems like computer vision, speech recognition or natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-mistress",
   "metadata": {},
   "source": [
    "**In problems involving large amounts of structured data, ML systems are typically superior to humans. However, it is much harder for ML systems to surpass humans in natural perception problems.** However, ML systems can and even have surpassed humans in certain problems of speech recognition, computer vision and medical examination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-original",
   "metadata": {},
   "source": [
    "### Improving your model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-story",
   "metadata": {},
   "source": [
    "To improve the performance of our ML model we should first try to identify if we're dealing with a high-bias or high-variance problem. Then, we should try to solve the problem using the standard solutions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-rouge",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/imp_m.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-vacuum",
   "metadata": {},
   "source": [
    "## Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-latvia",
   "metadata": {},
   "source": [
    "### Carrying out error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-desperate",
   "metadata": {},
   "source": [
    "Error analysis means manually investigating the mistakes of our model. It is possible to improve the model this way as long as the model is below human-level performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-ticket",
   "metadata": {},
   "source": [
    "**Typically, during error analysis we manually check how many dev set images were classified wrong and why. Then, we can decide if it is worth it to try and improve the classification.** For example, if 50/100 (5/100) incorrectly classified images were labelled as dogs instead of cats, it's probably (not) worth it to make the classifier better at differentiating between cats and dogs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-research",
   "metadata": {},
   "source": [
    "It can also be a good idea to evaluate multiple improvement ideas simultaneously while skimming through the incorrectly labelled dev set examples. E.g. checking which images could be fixed by improving detection of dogs/blurry pictures/lions... Then, we could **focus on improving errors on the idea with the highest ceiling (improvement % in detection).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-stephen",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/er_an.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-salem",
   "metadata": {},
   "source": [
    "### Cleaning up incorrectly labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-tuning",
   "metadata": {},
   "source": [
    "It is naturally always good to have correct labels. However, sometimes correcting erronous labels is not worth the time and effort relative to how much it increases the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-sympathy",
   "metadata": {},
   "source": [
    "*Training set*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-gazette",
   "metadata": {},
   "source": [
    "**DL algorithms are quite robust to random errors (incorrectly labelled data) in the training set.** So if the errors are more or less random (random misclicks by labeller etc.), it's usually fine to leave them as they are. Of course they can be fixed, but this will take time and the algorithm might be just fine with a few random errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-toronto",
   "metadata": {},
   "source": [
    "**However, systematic errors in the training set can mess up the algorithms.** So, if almost every white dog is incorrectly classified as a cat, this will cause problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-script",
   "metadata": {},
   "source": [
    "*Dev set*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-operator",
   "metadata": {},
   "source": [
    "During manual error analysis, we can check which images were wrongly labelled. Then, we can check the percentage of errors (like previously for dogs/blurs/lions...) and figure out if it's worth the time to correct them or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-password",
   "metadata": {},
   "source": [
    "The purpose of the dev set is to help us decide between different model types. If the errors are commonplace enough to hinder this, they should be corrected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-phoenix",
   "metadata": {},
   "source": [
    "*Correcting dev/test sets*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-effects",
   "metadata": {},
   "source": [
    "* **If we decide to correct the dev and test set labels, we should correct them using the same process, so they continue to be of the same distribution.** \n",
    "* It is good to remember there might be false positives (examples the algorithm got right incorrectly), not just false negatives, even though going through FPs could be very time consuming. \n",
    "* Correcting just dev/test but not training will cause them to come from slightly different distributions. This is generally ok."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-honolulu",
   "metadata": {},
   "source": [
    "Sometimes researchers make it sound like there is no need for human involvement in DL, just feed the data and let the model do its thing. Some even talk about this in a negative light, as in you should not inspect the errors manually. However, according to Ng, this is silly talk. While tedious, it can save a lot of time in the long run to take a look at the errors and try to understand where the model is going wrong. **So manually inspecting errors and using this knowledge in the model development is totally ok.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-classification",
   "metadata": {},
   "source": [
    "### Build your first system quickly, then iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-holder",
   "metadata": {},
   "source": [
    "When working on a new ML application, it's a good idea to set up a dev/test set and evaluation metric quickly. Then, build the initial system and start iterating. **Having an initial system (even a simple one) allows us to always choose the next step systematically based on bias/variance & error analysis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-macro",
   "metadata": {},
   "source": [
    "If the problem is well-known, it can be built with more care based on existing academic literature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-passing",
   "metadata": {},
   "source": [
    "### **Mismatched training and dev/test set**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-portsmouth",
   "metadata": {},
   "source": [
    "### Training and testing on different distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-mainstream",
   "metadata": {},
   "source": [
    "Since DL requires great amounts of training data, it is becoming increasingly common to shove web-scraped etc. data into the training set. This typically leads to a training and dev/test set that come from different distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-waste",
   "metadata": {},
   "source": [
    "**We should keep in mind that the dev/test sets should represent the (target) data we want the model to work on in the future.** So for example, if we want to make a mobile app for recognizing user submitted cat images, our dev/test sets should consist of those images. We can add some user images to the training set, but the training set can also contain a large amount of web-scraped cat images. However, we should not add any web scraped data into dev/test, since our target is the users' amateurish images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-baptist",
   "metadata": {},
   "source": [
    "In a nutshell, **it is ok to have different distributions for training and dev/test. Using a lot of data for training will improve the model** compared to trying to train with whatever little data is available from the dev/test distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-second",
   "metadata": {},
   "source": [
    "### Bias and Variance with mismatched data distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-stanley",
   "metadata": {},
   "source": [
    "Analyzing bias/variance helps us prioritize what to work on next. But the way we analyze them changes, if our training set comes from a different distribution than the dev/test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-antigua",
   "metadata": {},
   "source": [
    "**During analysis, we usually compare the training error and the dev error. However, when the distributions do not match, we can no longer make conclusions based on the error percentages.** For example, if the training error is 1 % and dev error 10 %, we cannot say for sure if the difference is caused by: \n",
    "1) The algorithm not generalizing well to the dev set (variance problem).  \n",
    "or  \n",
    "2) The dev set distribution having images that are harder to classify (data mismatch problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-death",
   "metadata": {},
   "source": [
    "**To solve this problem we define a *training-dev set*. It has the same distribution as the training set, but it is not used for training the NN.** So the training-dev set is a piece carved out of the training set and set aside during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-giant",
   "metadata": {},
   "source": [
    "Now, **we can inspect the error on the training set, the training-dev set and the dev set.** For example: \n",
    "* if the respective errors are 1 %, 9 % and 10 %, we can conclude that there is a variance problem (overfitting), since the trained NN does not generalize well to unseen data of the same distribution.  \n",
    "* if the errors are 10 %, 11 % and 12 %, and we know that human-level error is 0 %, we can conclude that there is an avoidable bias problem.  \n",
    "* if the errors are 1 %, 1.5 % and 10 %, we can conclude that there is a data mismatch problem, since the NN does generalize well to data of the same distribution but it does not do well on the dev set distribution that we care about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-prediction",
   "metadata": {},
   "source": [
    "We can also have combinations of the above scenarios such as 10 %, 11 % and 20 %, which would be an avoidable bias + data mismatch problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-delhi",
   "metadata": {},
   "source": [
    "Or, we might have 7 %, 10 % and 6 %, which would be a data mismatch problem where the dev set is easier to classify than the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-candle",
   "metadata": {},
   "source": [
    "For a more general approach, see the image. The stuff in the red box matters the most, but the other dev set numbers can help too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-calibration",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/bw_mm.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-effects",
   "metadata": {},
   "source": [
    "### Addressing data mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-antenna",
   "metadata": {},
   "source": [
    "We have learned methods for solving bias/variance problems, but the data mismatch problem is a new one. There are no completely systematic approaches for solving this problem, but there are some suggested methods we can try."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-photography",
   "metadata": {},
   "source": [
    "**Manual error analysis can help us identify the difference between training and dev set**, e.g. maybe the dev set has more noise than training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-politics",
   "metadata": {},
   "source": [
    "**Once we have identified the difference, we can collect more training data similar to dev data or try to make the sets more similar**, e.g. by adding simulated noise to the training data. The latter is called *artificial data synthesis*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-facility",
   "metadata": {},
   "source": [
    "**We can use artificial data synthesis to e.g. combine clear audio with car noise to create \"in-car audio\".** However, if the duration of our clear audio is much longer than the car noise, we cannot just loop the car noise. Looping the audio might sound fine to the human ear, but a NN will notice the repetitiveness, which might cause overfitting to this specific type of car noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-bouquet",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/adm.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-republican",
   "metadata": {},
   "source": [
    "In image recognition, artificial data synthesis can mean e.g. that we use computer generated images of cars. While this works, we should again be careful that we have enough different cars, so that the NN won't overfit to a small subset of all cars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-conversation",
   "metadata": {},
   "source": [
    "**Artificial data synthesis does work, we just need to be vary that we are not simulating data from a small subset of all possible cases.** This can cause overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-hampshire",
   "metadata": {},
   "source": [
    "### **Learning from multiple tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-camcorder",
   "metadata": {},
   "source": [
    "### Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-claim",
   "metadata": {},
   "source": [
    "**Sometimes we can transfer the knowledge gathered by a neural network from one task to another.** For example, maybe we can take a NN taught to recognize cats and use (some of) that NN's knowledge to help us read X-ray scans. This is a powerful DL trick called *transfer learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-signal",
   "metadata": {},
   "source": [
    "In the simplest case, we can just remove the output layer and its weights and replace them with a new output layer with randomly initialized weights. Then, we can use our new data (x, y) to:\n",
    "* retrain the weights of the output layer or the last few layers, if the new dataset is small.   \n",
    "* retrain all the weights in the NN, if the new dataset is large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-frontier",
   "metadata": {},
   "source": [
    "If we retrain all the weights, we use the term *pre-training* to describe the training done with the old dataset and the term *fine-tuning* to describe the training with the new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-microwave",
   "metadata": {},
   "source": [
    "**Transfer learning works, because the earlier layers typically recognize low-level features like edges and curves.** This knowledge about \"how images generally look like\" can then be applied to almost any image-recognition task. Naturally, the same principle also applies to speech recognition and other fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-machinery",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/tl.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-contents",
   "metadata": {},
   "source": [
    "Transfer learning **makes sense, when:**  \n",
    "* we transfer from a task with a lot of data (1,000,000 examples) to a task with little data (100 examples). The other way around does not make sense.  \n",
    "* both tasks have the same input x.  \n",
    "* low level features from the first task could be helpful for learning the second task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-bumper",
   "metadata": {},
   "source": [
    "Transfer learning is a very commonly used method in DL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-escape",
   "metadata": {},
   "source": [
    "### Multi-task learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-seating",
   "metadata": {},
   "source": [
    "In transfer learning, we learn one task and sequentially transfer this knowledge to another task. However, in *multi-task learning* **we have a NN learn multiple tasks simultaneously. Each of these learned tasks helps the NN learn the other tasks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-running",
   "metadata": {},
   "source": [
    "For example, **an autonomous vehicle needs to learn to simultaneously recognize pedestrians, cars, stop signs and traffic lights.** The corresponding NN would have an output layer of 4 neurons with values {0, 1}, one for recognizing each object. Each image fed to such a network can have multiple labels (car + pedestrian + traffic light). Training such a network is an example of multi-task learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-characterization",
   "metadata": {},
   "source": [
    "In multi-task learning we are building a single NN that is solving multiple tasks at the same time. The alternative would be building a separate NN for each task. However, multi-task learning is much more efficient as long as the earlier layers in the NN are useful for all the learned tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-child",
   "metadata": {},
   "source": [
    "Multi-task learning works even if some of the output labels are missing for some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-subscription",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/mtl.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-uruguay",
   "metadata": {},
   "source": [
    "Multi-task learning **makes sense, when:**  \n",
    "* training on a set of tasks that could benefit from having shared lower-level features.  \n",
    "* the amount of data we have for each task is similar (not a hard rule). For example, having 100 tasks with 1000 examples each will allow all the tasks to essentially have 100,000 examples - the tasks boost each other's performance.\n",
    "* we are able to train a NN that is big enough to do well on the all tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-samoa",
   "metadata": {},
   "source": [
    "In practice, setting up multi-task learning can take a lot of effort, since it can be difficult to define so many tasks to train simultaneously on. Consequently, **multi-task learning is used much less often than transfer learning.** Nevertheless, in fields like computer vision multi-task learning can be very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-portuguese",
   "metadata": {},
   "source": [
    "### **End-to-end deep learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-prize",
   "metadata": {},
   "source": [
    "### What is end-to-end deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-variable",
   "metadata": {},
   "source": [
    "Certain data processing systems have required pipelines with many individual steps. *End-to-end deep learning* **turns a pipeline into a huge neural network that directly maps the inputs to the outputs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-variable",
   "metadata": {},
   "source": [
    "**When E2E works, it works really well and greatly simplifies the system** since individual hand-designed components are not needed. However, **it does not work for all problems.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-relevance",
   "metadata": {},
   "source": [
    "End-to-end DL requires large amounts of data (10,000-100,000 h of audio) to really shine. For small datasets (3000 h), pipelines often work as well or better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-receipt",
   "metadata": {},
   "source": [
    "An E2E approach can also be partial - skipping some of the pipeline steps while keeping others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-eagle",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/e2e.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-summary",
   "metadata": {},
   "source": [
    "**A pure E2E approach is not always the best.** For example, to recognize people's identities in images it is typically better to split the process into two steps instead of using a single E2E NN. The first step finds the face in the image and the second step compares a zoom-in of the face to a database of faces. This works better than E2E, since there is **a lot of data available for training the individual steps but not as much for the direct E2E approach** (\"image -> identity\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-continuity",
   "metadata": {},
   "source": [
    "A pure E2E approach works better than a pipeline in, for example translating English to French. There is enough data available to do such an X -> Y task directly without any substeps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-procurement",
   "metadata": {},
   "source": [
    "Historically, accepting end-to-end deep learning as a viable option has been difficult for some researchers who have spent most of their scientific careers designing the individual pipeline steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-server",
   "metadata": {},
   "source": [
    "### Whether to use end-to-end deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-perfume",
   "metadata": {},
   "source": [
    "Here we go through the pros and cons of E2E DL to try and obtain a more systematic understanding of whether to use it or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-receiver",
   "metadata": {},
   "source": [
    "**Pros of E2E DL:**  \n",
    "* Lets the data do the speaking instead of trying to force the NN to think like humans (e.g. how words are supposed to be heard).\n",
    "* Less hand-designing of components needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-falls",
   "metadata": {},
   "source": [
    "**Cons of E2E DL:**  \n",
    "* May need large amounts of data.  \n",
    "* Excludes potentially useful hand-designed components. \n",
    "  * A learning algorithm has two main sources of knowledge: 1) data, 2) hand-designed components. If data is lacking, a well hand-designed algorithm can still work well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-buffalo",
   "metadata": {},
   "source": [
    "<img src=\"notes_images/we2e.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-orleans",
   "metadata": {},
   "source": [
    "For example in autonomous driving, E2E DL is not the most promising method. It is easier to split the problem into multiple subtasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
